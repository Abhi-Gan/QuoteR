{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages (from fastparquet) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages (from fastparquet) (1.26.3)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.8.1-cp39-cp39-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec in /Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages (from fastparquet) (2023.12.2)\n",
      "Requirement already satisfied: packaging in /Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages (from fastparquet) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading fastparquet-2024.2.0-cp39-cp39-macosx_11_0_arm64.whl (685 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m685.3/685.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cramjam-2.8.1-cp39-cp39-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.8.1 fastparquet-2024.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy #==1.19.5\n",
    "# !pip install torch #==1.7.1  \n",
    "# !pip install --upgrade pip setuptools\n",
    "# !pip install transformers\n",
    "# !pip install sklearn #==0.0\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install pyarrow\n",
    "# !pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/abhi/Desktop/Chaudhury_Lab/QuoteR_Imp/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel#, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertSememeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert import BertPreTrainedModel\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.models.bert.modeling_bert import BERT_INPUTS_DOCSTRING, _TOKENIZER_FOR_DOC, BertSememeEmbeddings, BertEncoder, BertPooler\n",
    "# TODO: add BertSemeModel\n",
    "class BertSememeModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well\n",
    "    as a decoder, in which case a layer of cross-attention is added between\n",
    "    the self-attention layers, following the architecture described in `Attention is all you need`_ by Ashish Vaswani,\n",
    "    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the\n",
    "    :obj:`is_decoder` argument of the configuration set to :obj:`True`; an\n",
    "    :obj:`encoder_hidden_states` is expected as an input to the forward pass.\n",
    "\n",
    "    .. _`Attention is all you need`:\n",
    "        https://arxiv.org/abs/1706.03762\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertSememeEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\" Prunes heads of the model.\n",
    "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "            See base class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    # # @add_start_docstrings_to_callable\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "    @add_code_sample_docstrings(processor_class=_TOKENIZER_FOR_DOC, checkpoint=\"bert-base-uncased\")\n",
    "    def forward(\n",
    "        self,\n",
    "        quote_ids,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "    Return:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        pooler_output (:obj:`torch.FloatTensor`: of shape :obj:`(batch_size, hidden_size)`):\n",
    "            Last layer hidden-state of the first token of the sequence (classification token)\n",
    "            further processed by a Linear layer and a Tanh activation function. The Linear\n",
    "            layer weights are trained from the next sentence prediction (classification)\n",
    "            objective during pre-training.\n",
    "\n",
    "            This output is usually *not* a good summary\n",
    "            of the semantic content of the input, you're often better with averaging or pooling\n",
    "            the sequence of hidden-states for the whole input sequence.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D ou 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            quote_ids=quote_ids, input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "            1:\n",
    "        ]  # add hidden_states and attentions if they are here\n",
    "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(42)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch = 4\n",
    "sample_num = 19\n",
    "learning_rate = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loading dataset\n",
    "def load_data(path):\n",
    "    # return words list and labels\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip().lower().split('\\t') for line in lines]\n",
    "        train_former = [line[0] for line in lines[:101171]]\n",
    "        train_quote = [line[1] for line in lines[:101171]]\n",
    "        train_latter = [line[2] for line in lines[:101171]]\n",
    "        valid_former = [line[0] for line in lines[101171:113942]]\n",
    "        valid_quote = [line[1] for line in lines[101171:113942]]\n",
    "        valid_latter = [line[2] for line in lines[101171:113942]]\n",
    "        test_former = [line[0] for line in lines[113942:]]\n",
    "        test_quote = [line[1] for line in lines[113942:]]\n",
    "        test_latter = [line[2] for line in lines[113942:]]\n",
    "        all_quotes = train_quote + valid_quote + test_quote\n",
    "    # 去重\n",
    "    all_quotes = list(set(all_quotes))\n",
    "    all_quotes.sort()\n",
    "    y_train = [all_quotes.index(q) for q in train_quote]\n",
    "    y_valid = [all_quotes.index(q) for q in valid_quote]\n",
    "    y_test = [all_quotes.index(q) for q in test_quote]\n",
    "\n",
    "    return train_former, train_latter, train_quote, valid_former, valid_latter, valid_quote, test_former, test_latter, test_quote, torch.LongTensor(\n",
    "        y_train), torch.LongTensor(y_valid), torch.LongTensor(\n",
    "            y_test), all_quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset......\n",
      "train  valid  test: 101171 12771 12771\n",
      "all quotes:  6108\n",
      "train quote: 6008\n",
      "valid quote: 6108\n",
      "test quote: 6108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 19.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"loading dataset......\")\n",
    "data_path = \"./data/english.txt\"\n",
    "train_former, train_latter, train_quote, valid_former, valid_latter, valid_quote, test_former, test_latter, test_quote, y_train, y_valid, y_test, all_quotes = load_data(\n",
    "    data_path)\n",
    "print(\"train  valid  test:\", len(train_former), len(valid_former), len(test_former))\n",
    "print(\"all quotes: \", len(all_quotes))\n",
    "print(\"train quote:\", len(list(set(train_quote))))\n",
    "print(\"valid quote:\", len(list(set(valid_quote))))\n",
    "print(\"test quote:\", len(list(set(test_quote))))\n",
    "\n",
    "# get the Tokenizer used for pretraining model\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126713 total contexts in dset\n"
     ]
    }
   ],
   "source": [
    "# how many total contexts?\n",
    "print(f\"{101171 + 12771 + 12771} total contexts in dset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be done? is the first question. how is it to be done is a question which is secondary and its discussion is useless until the first is settled. too much state drove ginx's baby into the thames. what's \n",
      "everybody's business is nobody's business.\n",
      " if the uncountable babies of innumerable ginx's are to be aided, some one must aid them for the mere pleasure there is in loving-kindness. a baby is a human being, not a problem. a baby can't be explained away\n"
     ]
    }
   ],
   "source": [
    "print(train_former[1])\n",
    "print(train_quote[1])\n",
    "print(train_latter[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_tensors(former, latter):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_masks = []\n",
    "    mask_ids = []\n",
    "    for f, l in zip(former, latter):\n",
    "        sent = f + \"[MASK]\" + l\n",
    "        encoded_dict = tokenizer.encode_plus(sent,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=150,\n",
    "                                             padding='max_length',  # instead of pad_to_max_length=True\n",
    "                                             truncation=True,\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_tensors='pt')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        mask_index = encoded_dict['input_ids'][0].tolist().index(103)\n",
    "        mask_ids.append(mask_index)\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    return input_ids, token_type_ids, attention_masks, torch.LongTensor(\n",
    "        mask_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train and valid data......\n",
      "train bert input:\n",
      "torch.Size([101171, 150]) torch.Size([101171, 150]) torch.Size([101171, 150]) torch.Size([101171])\n",
      "valid bert input:\n",
      "torch.Size([12771, 150]) torch.Size([12771, 150]) torch.Size([12771, 150]) torch.Size([12771])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"loading train and valid data......\")\n",
    "train_input_ids, train_token_type_ids, train_attention_masks, train_mask_ids = make_context_tensors(\n",
    "    train_former, train_latter)\n",
    "valid_input_ids, valid_token_type_ids, valid_attention_masks, valid_mask_ids = make_context_tensors(\n",
    "    valid_former, valid_latter)\n",
    "print(\"train bert input:\")\n",
    "print(train_input_ids.shape, train_token_type_ids.shape,\n",
    "      train_attention_masks.shape, train_mask_ids.shape)\n",
    "print(\"valid bert input:\")\n",
    "print(valid_input_ids.shape, valid_token_type_ids.shape,\n",
    "      valid_attention_masks.shape, valid_mask_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train and valid dataloader ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dataset and DataLoader\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, input_ids, token_type_ids, attention_masks, mask_ids,\n",
    "                 quote):\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.mask_ids = mask_ids\n",
    "        self.quote = quote\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.quote is None:\n",
    "            return self.input_ids[idx], self.token_type_ids[\n",
    "                idx], self.attention_masks[idx], self.mask_ids[idx]\n",
    "        return self.input_ids[idx], self.token_type_ids[\n",
    "            idx], self.attention_masks[idx], self.mask_ids[idx], self.quote[\n",
    "                idx]\n",
    "\n",
    "\n",
    "print(\"loading train and valid dataloader ...\")\n",
    "train_dataset = Dataset(input_ids=train_input_ids,\n",
    "                        token_type_ids=train_token_type_ids,\n",
    "                        attention_masks=train_attention_masks,\n",
    "                        mask_ids=train_mask_ids,\n",
    "                        quote=train_quote)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0) # 2)\n",
    "valid_dataset = Dataset(input_ids=valid_input_ids,\n",
    "                        token_type_ids=valid_token_type_ids,\n",
    "                        attention_masks=valid_attention_masks,\n",
    "                        mask_ids=valid_mask_ids,\n",
    "                        quote=valid_quote)\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=batch,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0) # 2)\n",
    "\n",
    "\n",
    "#  generat negative examples according to num\n",
    "def generate_quotes(quote, num):\n",
    "    quotes_selcet = all_quotes[:]\n",
    "    quotes_selcet.remove(quote)\n",
    "    quotes = random.sample(quotes_selcet, num)\n",
    "    quotes.append(quote)\n",
    "    random.shuffle(quotes)\n",
    "    return quotes\n",
    "\n",
    "\n",
    "def make_quote_tensors(quote):\n",
    "    quotes = generate_quotes(quote, num=sample_num)\n",
    "    label = quotes.index(quote)\n",
    "    input_ids = []\n",
    "    for q in quotes:\n",
    "        encoded_dict = tokenizer.encode_plus(q,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=80,\n",
    "                                             padding='max_length',  # instead of pad_to_max_length=True\n",
    "                                             truncation=True,\n",
    "                                             return_tensors='pt')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "    input_ids = torch.cat(input_ids, 0)  # [num, 80]\n",
    "    quote_ids = [all_quotes.index(q) for q in quotes]\n",
    "    return input_ids, label, torch.LongTensor(quote_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define network\n",
    "class Context_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, context_input_ids, context_token_type_ids,\n",
    "                context_attention_masks, mask_ids):\n",
    "        outputs = self.bert_model(input_ids=context_input_ids,\n",
    "                                  token_type_ids=context_token_type_ids,\n",
    "                                  attention_mask=context_attention_masks)\n",
    "        last_hidden_state = outputs[0]  # [batch_size, sequence_length, hidden_size]\n",
    "        all_context = []\n",
    "        for i in range(len(last_hidden_state)):\n",
    "            hidden_state = last_hidden_state[i]  # [sequence_length, hidden_size]\n",
    "            mask = hidden_state[mask_ids[i]]\n",
    "            mask = self.dropout(mask)\n",
    "            context = mask.unsqueeze(dim=0)  # context: [1, hidden_size]\n",
    "            all_context.append(context)\n",
    "        all_context = torch.cat(all_context, dim=0)  # all_context: [batch, hidden_size]\n",
    "        return all_context\n",
    "\n",
    "\n",
    "class Quote_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_model = BertSememeModel.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "        # self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, quotes):\n",
    "        quote_tensor = []\n",
    "        labels = []\n",
    "        for quote in quotes:\n",
    "            quote_input_ids, label, quote_ids = make_quote_tensors(quote)\n",
    "            quote_input_ids = quote_input_ids.to(device)\n",
    "            quote_ids = quote_ids.to(device)\n",
    "            outputs = self.bert_model(input_ids=quote_input_ids, quote_ids=quote_ids)\n",
    "            last_hidden_state = outputs[0]  # (num, sequence_length, hidden_size))\n",
    "            # last_hidden_state = self.dropout(last_hidden_state)\n",
    "            output = torch.mean(last_hidden_state, dim=1)  # (num, hidden_size))\n",
    "            quote_tensor.append(output)\n",
    "            labels.append(label)\n",
    "        quote_tensor = torch.stack(quote_tensor, dim=0)  # (batch, num, hidden_size))\n",
    "        return quote_tensor, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuotRec_Net(nn.Module):\n",
    "    def __init__(self, contex_model, quote_model):\n",
    "        super().__init__()\n",
    "        self.contex_model = contex_model\n",
    "        self.quote_model = quote_model\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_masks, mask_ids,\n",
    "                quotes):\n",
    "        # context_output: [batch, hidden_size]\n",
    "        context_output = self.contex_model(input_ids, token_type_ids,\n",
    "                                           attention_masks, mask_ids)\n",
    "        context_output = context_output.unsqueeze(dim=1)  # [batch, 1, hidden_size]\n",
    "        # quote_output: [batch, num, hidden_size]  labels: [batch]\n",
    "        quote_output, labels = self.quote_model(quotes)\n",
    "        quote_output = quote_output.permute(0, 2, 1)\n",
    "        outputs = torch.matmul(context_output, quote_output).squeeze(\n",
    "            dim=1)  # output: [batch, num_quotes]\n",
    "        return outputs, torch.LongTensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertSememeModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.sememe_embeddings.lut.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuotRec_Net(\n",
       "  (contex_model): Context_Encoder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (quote_model): Quote_Encoder(\n",
       "    (bert_model): BertSememeModel(\n",
       "      (embeddings): BertSememeEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (sememe_embeddings): SememeEmbeddings(\n",
       "          (lut): Embedding(1726, 768)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"loading model......\")\n",
    "contex_model = Context_Encoder()\n",
    "quote_model = Quote_Encoder()\n",
    "model = QuotRec_Net(contex_model, quote_model)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training(model, epoch, train, valid, device):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(\n",
    "        total, trainable))\n",
    "    t_batch = len(train)\n",
    "    v_batch = len(valid)\n",
    "    # NOTE: the lengths of the datasets are too long\n",
    "    # in training takes ~11 sec for each data point\n",
    "    # say i want max 0.5 hr\n",
    "    max_num_data = int(0.5*60*60/11)\n",
    "    print(f\"max # data: {max_num_data}\")\n",
    "    if t_batch > max_num_data:\n",
    "        # train = train[0:max_num_data]\n",
    "        t_batch = max_num_data\n",
    "    \n",
    "    if v_batch > max_num_data:\n",
    "        # valid = valid[0:max_num_data]\n",
    "        v_batch = max_num_data\n",
    "\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    best_acc = 0\n",
    "    count = 0\n",
    "    for epoch in range(epoch):\n",
    "        start = time.perf_counter()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        print(\"epoch: \", epoch + 1)\n",
    "        # train\n",
    "        for i, (input_ids, token_type_ids, attention_masks, mask_ids,\n",
    "                quotes) in enumerate(train):\n",
    "            if i >= t_batch:\n",
    "                break\n",
    "\n",
    "            print(f\"On data {i}/{len(train)}\")\n",
    "            input_ids = input_ids.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            mask_ids = mask_ids.to(device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, labels = model(input_ids, token_type_ids, attention_masks,\n",
    "                                    mask_ids, quotes)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, pred = torch.max(outputs.cpu().data, 1)\n",
    "            acc = accuracy_score(pred, labels.cpu())\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "        print('Train | Loss:{:.5f} Acc:{:.3f}'.format(total_loss,\n",
    "                                                      total_acc / t_batch))\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (input_ids, token_type_ids, attention_masks, mask_ids,\n",
    "                    quotes) in enumerate(valid):\n",
    "                if i >= v_batch:\n",
    "                    break\n",
    "                \n",
    "                print(f\"On data {i}/{len(valid)}\")\n",
    "\n",
    "                input_ids = input_ids.to(device)\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "                attention_masks = attention_masks.to(device)\n",
    "                mask_ids = mask_ids.to(device, dtype=torch.long)\n",
    "                outputs, labels = model(input_ids, token_type_ids,\n",
    "                                        attention_masks, mask_ids, quotes)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, pred = torch.max(outputs.cpu().data, 1)\n",
    "                acc = accuracy_score(pred, labels.cpu())\n",
    "                total_loss += loss.item()\n",
    "                total_acc += acc\n",
    "            print('Valid | Loss:{:.5f} Acc:{:.3f}'.format(\n",
    "                total_loss, total_acc / v_batch))\n",
    "            if total_acc > best_acc:\n",
    "                best_acc = total_acc\n",
    "                if os.path.exists(\"./model\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    os.mkdir(\"./model\")\n",
    "                torch.save(\n",
    "                    model.quote_model.state_dict(), \"./model/english_quote.pth\")\n",
    "                torch.save(\n",
    "                    model.contex_model.state_dict(), \"./model/english_context.pth\")\n",
    "                print('saving model with Acc {:.3f} '.format(total_acc / v_batch))\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "        model.train()\n",
    "        end = time.perf_counter()\n",
    "        print('epoch running time:{:.0f}s'.format(end - start))\n",
    "        # early stopping\n",
    "        if count == 3:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training, parameter total:220290048, trainable:220290048\n",
      "\n",
      "max # data: 163\n",
      "epoch:  1\n",
      "On data 0/25293\n",
      "On data 1/25293\n",
      "On data 2/25293\n",
      "On data 3/25293\n",
      "On data 4/25293\n",
      "On data 5/25293\n",
      "On data 6/25293\n",
      "On data 7/25293\n",
      "On data 8/25293\n",
      "On data 9/25293\n",
      "On data 10/25293\n",
      "On data 11/25293\n",
      "On data 12/25293\n",
      "On data 13/25293\n",
      "On data 14/25293\n",
      "On data 15/25293\n",
      "On data 16/25293\n",
      "On data 17/25293\n",
      "On data 18/25293\n",
      "On data 19/25293\n",
      "On data 20/25293\n",
      "On data 21/25293\n",
      "On data 22/25293\n",
      "On data 23/25293\n",
      "On data 24/25293\n",
      "On data 25/25293\n",
      "On data 26/25293\n",
      "On data 27/25293\n",
      "On data 28/25293\n",
      "On data 29/25293\n",
      "On data 30/25293\n",
      "On data 31/25293\n",
      "On data 32/25293\n",
      "On data 33/25293\n",
      "On data 34/25293\n",
      "On data 35/25293\n",
      "On data 36/25293\n",
      "On data 37/25293\n",
      "On data 38/25293\n",
      "On data 39/25293\n",
      "On data 40/25293\n",
      "On data 41/25293\n",
      "On data 42/25293\n",
      "On data 43/25293\n",
      "On data 44/25293\n",
      "On data 45/25293\n",
      "On data 46/25293\n",
      "On data 47/25293\n",
      "On data 48/25293\n",
      "On data 49/25293\n",
      "On data 50/25293\n",
      "On data 51/25293\n",
      "On data 52/25293\n",
      "On data 53/25293\n",
      "On data 54/25293\n",
      "On data 55/25293\n",
      "On data 56/25293\n",
      "On data 57/25293\n",
      "On data 58/25293\n",
      "On data 59/25293\n",
      "On data 60/25293\n",
      "On data 61/25293\n",
      "On data 62/25293\n",
      "On data 63/25293\n",
      "On data 64/25293\n",
      "On data 65/25293\n",
      "On data 66/25293\n",
      "On data 67/25293\n",
      "On data 68/25293\n",
      "On data 69/25293\n",
      "On data 70/25293\n",
      "On data 71/25293\n",
      "On data 72/25293\n",
      "On data 73/25293\n",
      "On data 74/25293\n",
      "On data 75/25293\n",
      "On data 76/25293\n",
      "On data 77/25293\n",
      "On data 78/25293\n",
      "On data 79/25293\n",
      "On data 80/25293\n",
      "On data 81/25293\n",
      "On data 82/25293\n",
      "On data 83/25293\n",
      "On data 84/25293\n",
      "On data 85/25293\n",
      "On data 86/25293\n",
      "On data 87/25293\n",
      "On data 88/25293\n",
      "On data 89/25293\n",
      "On data 90/25293\n",
      "On data 91/25293\n",
      "On data 92/25293\n",
      "On data 93/25293\n",
      "On data 94/25293\n",
      "On data 95/25293\n",
      "On data 96/25293\n",
      "On data 97/25293\n",
      "On data 98/25293\n",
      "On data 99/25293\n",
      "On data 100/25293\n",
      "On data 101/25293\n",
      "On data 102/25293\n",
      "On data 103/25293\n",
      "On data 104/25293\n",
      "On data 105/25293\n",
      "On data 106/25293\n",
      "On data 107/25293\n",
      "On data 108/25293\n",
      "On data 109/25293\n",
      "On data 110/25293\n",
      "On data 111/25293\n",
      "On data 112/25293\n",
      "On data 113/25293\n",
      "On data 114/25293\n",
      "On data 115/25293\n",
      "On data 116/25293\n",
      "On data 117/25293\n",
      "On data 118/25293\n",
      "On data 119/25293\n",
      "On data 120/25293\n",
      "On data 121/25293\n",
      "On data 122/25293\n",
      "On data 123/25293\n",
      "On data 124/25293\n",
      "On data 125/25293\n",
      "On data 126/25293\n",
      "On data 127/25293\n",
      "On data 128/25293\n",
      "On data 129/25293\n",
      "On data 130/25293\n",
      "On data 131/25293\n",
      "On data 132/25293\n",
      "On data 133/25293\n",
      "On data 134/25293\n",
      "On data 135/25293\n",
      "On data 136/25293\n",
      "On data 137/25293\n",
      "On data 138/25293\n",
      "On data 139/25293\n",
      "On data 140/25293\n",
      "On data 141/25293\n",
      "On data 142/25293\n",
      "On data 143/25293\n",
      "On data 144/25293\n",
      "On data 145/25293\n",
      "On data 146/25293\n",
      "On data 147/25293\n",
      "On data 148/25293\n",
      "On data 149/25293\n",
      "On data 150/25293\n",
      "On data 151/25293\n",
      "On data 152/25293\n",
      "On data 153/25293\n",
      "On data 154/25293\n",
      "On data 155/25293\n",
      "On data 156/25293\n",
      "On data 157/25293\n",
      "On data 158/25293\n",
      "On data 159/25293\n",
      "On data 160/25293\n",
      "On data 161/25293\n",
      "On data 162/25293\n",
      "Train | Loss:234.20198 Acc:0.583\n",
      "On data 0/3193\n",
      "On data 1/3193\n",
      "On data 2/3193\n",
      "On data 3/3193\n",
      "On data 4/3193\n",
      "On data 5/3193\n",
      "On data 6/3193\n",
      "On data 7/3193\n",
      "On data 8/3193\n",
      "On data 9/3193\n",
      "On data 10/3193\n",
      "On data 11/3193\n",
      "On data 12/3193\n",
      "On data 13/3193\n",
      "On data 14/3193\n",
      "On data 15/3193\n",
      "On data 16/3193\n",
      "On data 17/3193\n",
      "On data 18/3193\n",
      "On data 19/3193\n",
      "On data 20/3193\n",
      "On data 21/3193\n",
      "On data 22/3193\n",
      "On data 23/3193\n",
      "On data 24/3193\n",
      "On data 25/3193\n",
      "On data 26/3193\n",
      "On data 27/3193\n",
      "On data 28/3193\n",
      "On data 29/3193\n",
      "On data 30/3193\n",
      "On data 31/3193\n",
      "On data 32/3193\n",
      "On data 33/3193\n",
      "On data 34/3193\n",
      "On data 35/3193\n",
      "On data 36/3193\n",
      "On data 37/3193\n",
      "On data 38/3193\n",
      "On data 39/3193\n",
      "On data 40/3193\n",
      "On data 41/3193\n",
      "On data 42/3193\n",
      "On data 43/3193\n",
      "On data 44/3193\n",
      "On data 45/3193\n",
      "On data 46/3193\n",
      "On data 47/3193\n",
      "On data 48/3193\n",
      "On data 49/3193\n",
      "On data 50/3193\n",
      "On data 51/3193\n",
      "On data 52/3193\n",
      "On data 53/3193\n",
      "On data 54/3193\n",
      "On data 55/3193\n",
      "On data 56/3193\n",
      "On data 57/3193\n",
      "On data 58/3193\n",
      "On data 59/3193\n",
      "On data 60/3193\n",
      "On data 61/3193\n",
      "On data 62/3193\n",
      "On data 63/3193\n",
      "On data 64/3193\n",
      "On data 65/3193\n",
      "On data 66/3193\n",
      "On data 67/3193\n",
      "On data 68/3193\n",
      "On data 69/3193\n",
      "On data 70/3193\n",
      "On data 71/3193\n",
      "On data 72/3193\n",
      "On data 73/3193\n",
      "On data 74/3193\n",
      "On data 75/3193\n",
      "On data 76/3193\n",
      "On data 77/3193\n",
      "On data 78/3193\n",
      "On data 79/3193\n",
      "On data 80/3193\n",
      "On data 81/3193\n",
      "On data 82/3193\n",
      "On data 83/3193\n",
      "On data 84/3193\n",
      "On data 85/3193\n",
      "On data 86/3193\n",
      "On data 87/3193\n",
      "On data 88/3193\n",
      "On data 89/3193\n",
      "On data 90/3193\n",
      "On data 91/3193\n",
      "On data 92/3193\n",
      "On data 93/3193\n",
      "On data 94/3193\n",
      "On data 95/3193\n",
      "On data 96/3193\n",
      "On data 97/3193\n",
      "On data 98/3193\n",
      "On data 99/3193\n",
      "On data 100/3193\n",
      "On data 101/3193\n",
      "On data 102/3193\n",
      "On data 103/3193\n",
      "On data 104/3193\n",
      "On data 105/3193\n",
      "On data 106/3193\n",
      "On data 107/3193\n",
      "On data 108/3193\n",
      "On data 109/3193\n",
      "On data 110/3193\n",
      "On data 111/3193\n",
      "On data 112/3193\n",
      "On data 113/3193\n",
      "On data 114/3193\n",
      "On data 115/3193\n",
      "On data 116/3193\n",
      "On data 117/3193\n",
      "On data 118/3193\n",
      "On data 119/3193\n",
      "On data 120/3193\n",
      "On data 121/3193\n",
      "On data 122/3193\n",
      "On data 123/3193\n",
      "On data 124/3193\n",
      "On data 125/3193\n",
      "On data 126/3193\n",
      "On data 127/3193\n",
      "On data 128/3193\n",
      "On data 129/3193\n",
      "On data 130/3193\n",
      "On data 131/3193\n",
      "On data 132/3193\n",
      "On data 133/3193\n",
      "On data 134/3193\n",
      "On data 135/3193\n",
      "On data 136/3193\n",
      "On data 137/3193\n",
      "On data 138/3193\n",
      "On data 139/3193\n",
      "On data 140/3193\n",
      "On data 141/3193\n",
      "On data 142/3193\n",
      "On data 143/3193\n",
      "On data 144/3193\n",
      "On data 145/3193\n",
      "On data 146/3193\n",
      "On data 147/3193\n",
      "On data 148/3193\n",
      "On data 149/3193\n",
      "On data 150/3193\n",
      "On data 151/3193\n",
      "On data 152/3193\n",
      "On data 153/3193\n",
      "On data 154/3193\n",
      "On data 155/3193\n",
      "On data 156/3193\n",
      "On data 157/3193\n",
      "On data 158/3193\n",
      "On data 159/3193\n",
      "On data 160/3193\n",
      "On data 161/3193\n",
      "On data 162/3193\n",
      "Valid | Loss:230.35519 Acc:0.578\n",
      "saving model with Acc 0.578 \n",
      "epoch running time:1912s\n",
      "epoch:  2\n",
      "On data 0/25293\n",
      "On data 1/25293\n",
      "On data 2/25293\n",
      "On data 3/25293\n",
      "On data 4/25293\n",
      "On data 5/25293\n",
      "On data 6/25293\n",
      "On data 7/25293\n",
      "On data 8/25293\n",
      "On data 9/25293\n",
      "On data 10/25293\n",
      "On data 11/25293\n",
      "On data 12/25293\n",
      "On data 13/25293\n",
      "On data 14/25293\n",
      "On data 15/25293\n",
      "On data 16/25293\n",
      "On data 17/25293\n",
      "On data 18/25293\n",
      "On data 19/25293\n",
      "On data 20/25293\n",
      "On data 21/25293\n",
      "On data 22/25293\n",
      "On data 23/25293\n",
      "On data 24/25293\n",
      "On data 25/25293\n",
      "On data 26/25293\n",
      "On data 27/25293\n",
      "On data 28/25293\n",
      "On data 29/25293\n",
      "On data 30/25293\n",
      "On data 31/25293\n",
      "On data 32/25293\n",
      "On data 33/25293\n",
      "On data 34/25293\n",
      "On data 35/25293\n",
      "On data 36/25293\n",
      "On data 37/25293\n",
      "On data 38/25293\n",
      "On data 39/25293\n",
      "On data 40/25293\n",
      "On data 41/25293\n",
      "On data 42/25293\n",
      "On data 43/25293\n",
      "On data 44/25293\n",
      "On data 45/25293\n",
      "On data 46/25293\n",
      "On data 47/25293\n",
      "On data 48/25293\n",
      "On data 49/25293\n",
      "On data 50/25293\n",
      "On data 51/25293\n",
      "On data 52/25293\n",
      "On data 53/25293\n",
      "On data 54/25293\n",
      "On data 55/25293\n",
      "On data 56/25293\n",
      "On data 57/25293\n",
      "On data 58/25293\n",
      "On data 59/25293\n",
      "On data 60/25293\n",
      "On data 61/25293\n",
      "On data 62/25293\n",
      "On data 63/25293\n",
      "On data 64/25293\n",
      "On data 65/25293\n",
      "On data 66/25293\n",
      "On data 67/25293\n",
      "On data 68/25293\n",
      "On data 69/25293\n",
      "On data 70/25293\n",
      "On data 71/25293\n",
      "On data 72/25293\n",
      "On data 73/25293\n",
      "On data 74/25293\n",
      "On data 75/25293\n",
      "On data 76/25293\n",
      "On data 77/25293\n",
      "On data 78/25293\n",
      "On data 79/25293\n",
      "On data 80/25293\n",
      "On data 81/25293\n",
      "On data 82/25293\n",
      "On data 83/25293\n",
      "On data 84/25293\n",
      "On data 85/25293\n",
      "On data 86/25293\n",
      "On data 87/25293\n",
      "On data 88/25293\n",
      "On data 89/25293\n",
      "On data 90/25293\n",
      "On data 91/25293\n",
      "On data 92/25293\n",
      "On data 93/25293\n",
      "On data 94/25293\n",
      "On data 95/25293\n",
      "On data 96/25293\n",
      "On data 97/25293\n",
      "On data 98/25293\n",
      "On data 99/25293\n",
      "On data 100/25293\n",
      "On data 101/25293\n",
      "On data 102/25293\n",
      "On data 103/25293\n",
      "On data 104/25293\n",
      "On data 105/25293\n",
      "On data 106/25293\n",
      "On data 107/25293\n",
      "On data 108/25293\n",
      "On data 109/25293\n",
      "On data 110/25293\n",
      "On data 111/25293\n",
      "On data 112/25293\n",
      "On data 113/25293\n",
      "On data 114/25293\n",
      "On data 115/25293\n",
      "On data 116/25293\n",
      "On data 117/25293\n",
      "On data 118/25293\n",
      "On data 119/25293\n",
      "On data 120/25293\n",
      "On data 121/25293\n",
      "On data 122/25293\n",
      "On data 123/25293\n",
      "On data 124/25293\n",
      "On data 125/25293\n",
      "On data 126/25293\n",
      "On data 127/25293\n",
      "On data 128/25293\n",
      "On data 129/25293\n",
      "On data 130/25293\n",
      "On data 131/25293\n",
      "On data 132/25293\n",
      "On data 133/25293\n",
      "On data 134/25293\n",
      "On data 135/25293\n",
      "On data 136/25293\n",
      "On data 137/25293\n",
      "On data 138/25293\n",
      "On data 139/25293\n",
      "On data 140/25293\n",
      "On data 141/25293\n",
      "On data 142/25293\n",
      "On data 143/25293\n",
      "On data 144/25293\n",
      "On data 145/25293\n",
      "On data 146/25293\n",
      "On data 147/25293\n",
      "On data 148/25293\n",
      "On data 149/25293\n",
      "On data 150/25293\n",
      "On data 151/25293\n",
      "On data 152/25293\n",
      "On data 153/25293\n",
      "On data 154/25293\n",
      "On data 155/25293\n",
      "On data 156/25293\n",
      "On data 157/25293\n",
      "On data 158/25293\n",
      "On data 159/25293\n",
      "On data 160/25293\n",
      "On data 161/25293\n",
      "On data 162/25293\n",
      "Train | Loss:223.65944 Acc:0.604\n",
      "On data 0/3193\n",
      "On data 1/3193\n",
      "On data 2/3193\n",
      "On data 3/3193\n",
      "On data 4/3193\n",
      "On data 5/3193\n",
      "On data 6/3193\n",
      "On data 7/3193\n",
      "On data 8/3193\n",
      "On data 9/3193\n",
      "On data 10/3193\n",
      "On data 11/3193\n",
      "On data 12/3193\n",
      "On data 13/3193\n",
      "On data 14/3193\n",
      "On data 15/3193\n",
      "On data 16/3193\n",
      "On data 17/3193\n",
      "On data 18/3193\n",
      "On data 19/3193\n",
      "On data 20/3193\n",
      "On data 21/3193\n",
      "On data 22/3193\n",
      "On data 23/3193\n",
      "On data 24/3193\n",
      "On data 25/3193\n",
      "On data 26/3193\n",
      "On data 27/3193\n",
      "On data 28/3193\n",
      "On data 29/3193\n",
      "On data 30/3193\n",
      "On data 31/3193\n",
      "On data 32/3193\n",
      "On data 33/3193\n",
      "On data 34/3193\n",
      "On data 35/3193\n",
      "On data 36/3193\n",
      "On data 37/3193\n",
      "On data 38/3193\n",
      "On data 39/3193\n",
      "On data 40/3193\n",
      "On data 41/3193\n",
      "On data 42/3193\n",
      "On data 43/3193\n",
      "On data 44/3193\n",
      "On data 45/3193\n",
      "On data 46/3193\n",
      "On data 47/3193\n",
      "On data 48/3193\n",
      "On data 49/3193\n",
      "On data 50/3193\n",
      "On data 51/3193\n",
      "On data 52/3193\n",
      "On data 53/3193\n",
      "On data 54/3193\n",
      "On data 55/3193\n",
      "On data 56/3193\n",
      "On data 57/3193\n",
      "On data 58/3193\n",
      "On data 59/3193\n",
      "On data 60/3193\n",
      "On data 61/3193\n",
      "On data 62/3193\n",
      "On data 63/3193\n",
      "On data 64/3193\n",
      "On data 65/3193\n",
      "On data 66/3193\n",
      "On data 67/3193\n",
      "On data 68/3193\n",
      "On data 69/3193\n",
      "On data 70/3193\n",
      "On data 71/3193\n",
      "On data 72/3193\n",
      "On data 73/3193\n",
      "On data 74/3193\n",
      "On data 75/3193\n",
      "On data 76/3193\n",
      "On data 77/3193\n",
      "On data 78/3193\n",
      "On data 79/3193\n",
      "On data 80/3193\n",
      "On data 81/3193\n",
      "On data 82/3193\n",
      "On data 83/3193\n",
      "On data 84/3193\n",
      "On data 85/3193\n",
      "On data 86/3193\n",
      "On data 87/3193\n",
      "On data 88/3193\n",
      "On data 89/3193\n",
      "On data 90/3193\n",
      "On data 91/3193\n",
      "On data 92/3193\n",
      "On data 93/3193\n",
      "On data 94/3193\n",
      "On data 95/3193\n",
      "On data 96/3193\n",
      "On data 97/3193\n",
      "On data 98/3193\n",
      "On data 99/3193\n",
      "On data 100/3193\n",
      "On data 101/3193\n",
      "On data 102/3193\n",
      "On data 103/3193\n",
      "On data 104/3193\n",
      "On data 105/3193\n",
      "On data 106/3193\n",
      "On data 107/3193\n",
      "On data 108/3193\n",
      "On data 109/3193\n",
      "On data 110/3193\n",
      "On data 111/3193\n",
      "On data 112/3193\n",
      "On data 113/3193\n",
      "On data 114/3193\n",
      "On data 115/3193\n",
      "On data 116/3193\n",
      "On data 117/3193\n",
      "On data 118/3193\n",
      "On data 119/3193\n",
      "On data 120/3193\n",
      "On data 121/3193\n",
      "On data 122/3193\n",
      "On data 123/3193\n",
      "On data 124/3193\n",
      "On data 125/3193\n",
      "On data 126/3193\n",
      "On data 127/3193\n",
      "On data 128/3193\n",
      "On data 129/3193\n",
      "On data 130/3193\n",
      "On data 131/3193\n",
      "On data 132/3193\n",
      "On data 133/3193\n",
      "On data 134/3193\n",
      "On data 135/3193\n",
      "On data 136/3193\n",
      "On data 137/3193\n",
      "On data 138/3193\n",
      "On data 139/3193\n",
      "On data 140/3193\n",
      "On data 141/3193\n",
      "On data 142/3193\n",
      "On data 143/3193\n",
      "On data 144/3193\n",
      "On data 145/3193\n",
      "On data 146/3193\n",
      "On data 147/3193\n",
      "On data 148/3193\n",
      "On data 149/3193\n",
      "On data 150/3193\n",
      "On data 151/3193\n",
      "On data 152/3193\n",
      "On data 153/3193\n",
      "On data 154/3193\n",
      "On data 155/3193\n",
      "On data 156/3193\n",
      "On data 157/3193\n",
      "On data 158/3193\n",
      "On data 159/3193\n",
      "On data 160/3193\n",
      "On data 161/3193\n",
      "On data 162/3193\n",
      "Valid | Loss:243.17300 Acc:0.549\n",
      "epoch running time:1676s\n",
      "epoch:  3\n",
      "On data 0/25293\n",
      "On data 1/25293\n",
      "On data 2/25293\n",
      "On data 3/25293\n",
      "On data 4/25293\n",
      "On data 5/25293\n",
      "On data 6/25293\n",
      "On data 7/25293\n",
      "On data 8/25293\n",
      "On data 9/25293\n",
      "On data 10/25293\n",
      "On data 11/25293\n",
      "On data 12/25293\n",
      "On data 13/25293\n",
      "On data 14/25293\n",
      "On data 15/25293\n",
      "On data 16/25293\n",
      "On data 17/25293\n",
      "On data 18/25293\n",
      "On data 19/25293\n",
      "On data 20/25293\n",
      "On data 21/25293\n",
      "On data 22/25293\n",
      "On data 23/25293\n",
      "On data 24/25293\n",
      "On data 25/25293\n",
      "On data 26/25293\n",
      "On data 27/25293\n",
      "On data 28/25293\n",
      "On data 29/25293\n",
      "On data 30/25293\n",
      "On data 31/25293\n",
      "On data 32/25293\n",
      "On data 33/25293\n",
      "On data 34/25293\n",
      "On data 35/25293\n",
      "On data 36/25293\n",
      "On data 37/25293\n",
      "On data 38/25293\n",
      "On data 39/25293\n",
      "On data 40/25293\n",
      "On data 41/25293\n",
      "On data 42/25293\n",
      "On data 43/25293\n",
      "On data 44/25293\n",
      "On data 45/25293\n",
      "On data 46/25293\n",
      "On data 47/25293\n",
      "On data 48/25293\n",
      "On data 49/25293\n",
      "On data 50/25293\n",
      "On data 51/25293\n",
      "On data 52/25293\n",
      "On data 53/25293\n",
      "On data 54/25293\n",
      "On data 55/25293\n",
      "On data 56/25293\n",
      "On data 57/25293\n",
      "On data 58/25293\n",
      "On data 59/25293\n",
      "On data 60/25293\n",
      "On data 61/25293\n",
      "On data 62/25293\n",
      "On data 63/25293\n",
      "On data 64/25293\n",
      "On data 65/25293\n",
      "On data 66/25293\n",
      "On data 67/25293\n",
      "On data 68/25293\n",
      "On data 69/25293\n",
      "On data 70/25293\n",
      "On data 71/25293\n",
      "On data 72/25293\n",
      "On data 73/25293\n",
      "On data 74/25293\n",
      "On data 75/25293\n",
      "On data 76/25293\n",
      "On data 77/25293\n",
      "On data 78/25293\n",
      "On data 79/25293\n",
      "On data 80/25293\n",
      "On data 81/25293\n",
      "On data 82/25293\n",
      "On data 83/25293\n",
      "On data 84/25293\n",
      "On data 85/25293\n",
      "On data 86/25293\n",
      "On data 87/25293\n",
      "On data 88/25293\n",
      "On data 89/25293\n",
      "On data 90/25293\n",
      "On data 91/25293\n",
      "On data 92/25293\n",
      "On data 93/25293\n",
      "On data 94/25293\n",
      "On data 95/25293\n",
      "On data 96/25293\n",
      "On data 97/25293\n",
      "On data 98/25293\n",
      "On data 99/25293\n",
      "On data 100/25293\n",
      "On data 101/25293\n",
      "On data 102/25293\n",
      "On data 103/25293\n",
      "On data 104/25293\n",
      "On data 105/25293\n",
      "On data 106/25293\n",
      "On data 107/25293\n",
      "On data 108/25293\n",
      "On data 109/25293\n",
      "On data 110/25293\n",
      "On data 111/25293\n",
      "On data 112/25293\n",
      "On data 113/25293\n",
      "On data 114/25293\n",
      "On data 115/25293\n",
      "On data 116/25293\n",
      "On data 117/25293\n",
      "On data 118/25293\n",
      "On data 119/25293\n",
      "On data 120/25293\n",
      "On data 121/25293\n",
      "On data 122/25293\n",
      "On data 123/25293\n",
      "On data 124/25293\n",
      "On data 125/25293\n",
      "On data 126/25293\n",
      "On data 127/25293\n",
      "On data 128/25293\n",
      "On data 129/25293\n",
      "On data 130/25293\n",
      "On data 131/25293\n",
      "On data 132/25293\n",
      "On data 133/25293\n",
      "On data 134/25293\n",
      "On data 135/25293\n",
      "On data 136/25293\n",
      "On data 137/25293\n",
      "On data 138/25293\n",
      "On data 139/25293\n",
      "On data 140/25293\n",
      "On data 141/25293\n",
      "On data 142/25293\n",
      "On data 143/25293\n",
      "On data 144/25293\n",
      "On data 145/25293\n",
      "On data 146/25293\n",
      "On data 147/25293\n",
      "On data 148/25293\n",
      "On data 149/25293\n",
      "On data 150/25293\n",
      "On data 151/25293\n",
      "On data 152/25293\n",
      "On data 153/25293\n",
      "On data 154/25293\n",
      "On data 155/25293\n",
      "On data 156/25293\n",
      "On data 157/25293\n",
      "On data 158/25293\n",
      "On data 159/25293\n",
      "On data 160/25293\n",
      "On data 161/25293\n",
      "On data 162/25293\n",
      "Train | Loss:222.17262 Acc:0.578\n",
      "On data 0/3193\n",
      "On data 1/3193\n",
      "On data 2/3193\n",
      "On data 3/3193\n",
      "On data 4/3193\n",
      "On data 5/3193\n",
      "On data 6/3193\n",
      "On data 7/3193\n",
      "On data 8/3193\n",
      "On data 9/3193\n",
      "On data 10/3193\n",
      "On data 11/3193\n",
      "On data 12/3193\n",
      "On data 13/3193\n",
      "On data 14/3193\n",
      "On data 15/3193\n",
      "On data 16/3193\n",
      "On data 17/3193\n",
      "On data 18/3193\n",
      "On data 19/3193\n",
      "On data 20/3193\n",
      "On data 21/3193\n",
      "On data 22/3193\n",
      "On data 23/3193\n",
      "On data 24/3193\n",
      "On data 25/3193\n",
      "On data 26/3193\n",
      "On data 27/3193\n",
      "On data 28/3193\n",
      "On data 29/3193\n",
      "On data 30/3193\n",
      "On data 31/3193\n",
      "On data 32/3193\n",
      "On data 33/3193\n",
      "On data 34/3193\n",
      "On data 35/3193\n",
      "On data 36/3193\n",
      "On data 37/3193\n",
      "On data 38/3193\n",
      "On data 39/3193\n",
      "On data 40/3193\n",
      "On data 41/3193\n",
      "On data 42/3193\n",
      "On data 43/3193\n",
      "On data 44/3193\n",
      "On data 45/3193\n",
      "On data 46/3193\n",
      "On data 47/3193\n",
      "On data 48/3193\n",
      "On data 49/3193\n",
      "On data 50/3193\n",
      "On data 51/3193\n",
      "On data 52/3193\n",
      "On data 53/3193\n",
      "On data 54/3193\n",
      "On data 55/3193\n",
      "On data 56/3193\n",
      "On data 57/3193\n",
      "On data 58/3193\n",
      "On data 59/3193\n",
      "On data 60/3193\n",
      "On data 61/3193\n",
      "On data 62/3193\n",
      "On data 63/3193\n",
      "On data 64/3193\n",
      "On data 65/3193\n",
      "On data 66/3193\n",
      "On data 67/3193\n",
      "On data 68/3193\n",
      "On data 69/3193\n",
      "On data 70/3193\n",
      "On data 71/3193\n",
      "On data 72/3193\n",
      "On data 73/3193\n",
      "On data 74/3193\n",
      "On data 75/3193\n",
      "On data 76/3193\n",
      "On data 77/3193\n",
      "On data 78/3193\n",
      "On data 79/3193\n",
      "On data 80/3193\n",
      "On data 81/3193\n",
      "On data 82/3193\n",
      "On data 83/3193\n",
      "On data 84/3193\n",
      "On data 85/3193\n",
      "On data 86/3193\n",
      "On data 87/3193\n",
      "On data 88/3193\n",
      "On data 89/3193\n",
      "On data 90/3193\n",
      "On data 91/3193\n",
      "On data 92/3193\n",
      "On data 93/3193\n",
      "On data 94/3193\n",
      "On data 95/3193\n",
      "On data 96/3193\n",
      "On data 97/3193\n",
      "On data 98/3193\n",
      "On data 99/3193\n",
      "On data 100/3193\n",
      "On data 101/3193\n",
      "On data 102/3193\n",
      "On data 103/3193\n",
      "On data 104/3193\n",
      "On data 105/3193\n",
      "On data 106/3193\n",
      "On data 107/3193\n",
      "On data 108/3193\n",
      "On data 109/3193\n",
      "On data 110/3193\n",
      "On data 111/3193\n",
      "On data 112/3193\n",
      "On data 113/3193\n",
      "On data 114/3193\n",
      "On data 115/3193\n",
      "On data 116/3193\n",
      "On data 117/3193\n",
      "On data 118/3193\n",
      "On data 119/3193\n",
      "On data 120/3193\n",
      "On data 121/3193\n",
      "On data 122/3193\n",
      "On data 123/3193\n",
      "On data 124/3193\n",
      "On data 125/3193\n",
      "On data 126/3193\n",
      "On data 127/3193\n",
      "On data 128/3193\n",
      "On data 129/3193\n",
      "On data 130/3193\n",
      "On data 131/3193\n",
      "On data 132/3193\n",
      "On data 133/3193\n",
      "On data 134/3193\n",
      "On data 135/3193\n",
      "On data 136/3193\n",
      "On data 137/3193\n",
      "On data 138/3193\n",
      "On data 139/3193\n",
      "On data 140/3193\n",
      "On data 141/3193\n",
      "On data 142/3193\n",
      "On data 143/3193\n",
      "On data 144/3193\n",
      "On data 145/3193\n",
      "On data 146/3193\n",
      "On data 147/3193\n",
      "On data 148/3193\n",
      "On data 149/3193\n",
      "On data 150/3193\n",
      "On data 151/3193\n",
      "On data 152/3193\n",
      "On data 153/3193\n",
      "On data 154/3193\n",
      "On data 155/3193\n",
      "On data 156/3193\n",
      "On data 157/3193\n",
      "On data 158/3193\n",
      "On data 159/3193\n",
      "On data 160/3193\n",
      "On data 161/3193\n",
      "On data 162/3193\n",
      "Valid | Loss:216.05787 Acc:0.604\n",
      "saving model with Acc 0.604 \n",
      "epoch running time:1602s\n",
      "epoch:  4\n",
      "On data 0/25293\n",
      "On data 1/25293\n",
      "On data 2/25293\n",
      "On data 3/25293\n",
      "On data 4/25293\n",
      "On data 5/25293\n",
      "On data 6/25293\n",
      "On data 7/25293\n",
      "On data 8/25293\n",
      "On data 9/25293\n",
      "On data 10/25293\n",
      "On data 11/25293\n",
      "On data 12/25293\n",
      "On data 13/25293\n",
      "On data 14/25293\n",
      "On data 15/25293\n",
      "On data 16/25293\n",
      "On data 17/25293\n",
      "On data 18/25293\n",
      "On data 19/25293\n",
      "On data 20/25293\n",
      "On data 21/25293\n",
      "On data 22/25293\n",
      "On data 23/25293\n",
      "On data 24/25293\n",
      "On data 25/25293\n",
      "On data 26/25293\n",
      "On data 27/25293\n",
      "On data 28/25293\n",
      "On data 29/25293\n",
      "On data 30/25293\n",
      "On data 31/25293\n",
      "On data 32/25293\n",
      "On data 33/25293\n",
      "On data 34/25293\n",
      "On data 35/25293\n",
      "On data 36/25293\n",
      "On data 37/25293\n",
      "On data 38/25293\n",
      "On data 39/25293\n",
      "On data 40/25293\n",
      "On data 41/25293\n",
      "On data 42/25293\n",
      "On data 43/25293\n",
      "On data 44/25293\n",
      "On data 45/25293\n",
      "On data 46/25293\n",
      "On data 47/25293\n",
      "On data 48/25293\n",
      "On data 49/25293\n",
      "On data 50/25293\n",
      "On data 51/25293\n",
      "On data 52/25293\n",
      "On data 53/25293\n",
      "On data 54/25293\n",
      "On data 55/25293\n",
      "On data 56/25293\n",
      "On data 57/25293\n",
      "On data 58/25293\n",
      "On data 59/25293\n",
      "On data 60/25293\n",
      "On data 61/25293\n",
      "On data 62/25293\n",
      "On data 63/25293\n",
      "On data 64/25293\n",
      "On data 65/25293\n",
      "On data 66/25293\n",
      "On data 67/25293\n",
      "On data 68/25293\n",
      "On data 69/25293\n",
      "On data 70/25293\n",
      "On data 71/25293\n",
      "On data 72/25293\n",
      "On data 73/25293\n",
      "On data 74/25293\n",
      "On data 75/25293\n",
      "On data 76/25293\n",
      "On data 77/25293\n",
      "On data 78/25293\n",
      "On data 79/25293\n",
      "On data 80/25293\n",
      "On data 81/25293\n",
      "On data 82/25293\n",
      "On data 83/25293\n",
      "On data 84/25293\n",
      "On data 85/25293\n",
      "On data 86/25293\n",
      "On data 87/25293\n",
      "On data 88/25293\n",
      "On data 89/25293\n",
      "On data 90/25293\n",
      "On data 91/25293\n",
      "On data 92/25293\n",
      "On data 93/25293\n",
      "On data 94/25293\n",
      "On data 95/25293\n",
      "On data 96/25293\n",
      "On data 97/25293\n",
      "On data 98/25293\n",
      "On data 99/25293\n",
      "On data 100/25293\n",
      "On data 101/25293\n",
      "On data 102/25293\n",
      "On data 103/25293\n",
      "On data 104/25293\n",
      "On data 105/25293\n",
      "On data 106/25293\n",
      "On data 107/25293\n",
      "On data 108/25293\n",
      "On data 109/25293\n",
      "On data 110/25293\n",
      "On data 111/25293\n",
      "On data 112/25293\n",
      "On data 113/25293\n",
      "On data 114/25293\n",
      "On data 115/25293\n",
      "On data 116/25293\n",
      "On data 117/25293\n",
      "On data 118/25293\n",
      "On data 119/25293\n",
      "On data 120/25293\n",
      "On data 121/25293\n",
      "On data 122/25293\n",
      "On data 123/25293\n",
      "On data 124/25293\n",
      "On data 125/25293\n",
      "On data 126/25293\n",
      "On data 127/25293\n",
      "On data 128/25293\n",
      "On data 129/25293\n",
      "On data 130/25293\n",
      "On data 131/25293\n",
      "On data 132/25293\n",
      "On data 133/25293\n",
      "On data 134/25293\n",
      "On data 135/25293\n",
      "On data 136/25293\n",
      "On data 137/25293\n",
      "On data 138/25293\n",
      "On data 139/25293\n",
      "On data 140/25293\n",
      "On data 141/25293\n",
      "On data 142/25293\n",
      "On data 143/25293\n",
      "On data 144/25293\n",
      "On data 145/25293\n",
      "On data 146/25293\n",
      "On data 147/25293\n",
      "On data 148/25293\n",
      "On data 149/25293\n",
      "On data 150/25293\n",
      "On data 151/25293\n",
      "On data 152/25293\n",
      "On data 153/25293\n",
      "On data 154/25293\n",
      "On data 155/25293\n",
      "On data 156/25293\n",
      "On data 157/25293\n",
      "On data 158/25293\n",
      "On data 159/25293\n",
      "On data 160/25293\n",
      "On data 161/25293\n",
      "On data 162/25293\n",
      "Train | Loss:204.58444 Acc:0.610\n",
      "On data 0/3193\n",
      "On data 1/3193\n",
      "On data 2/3193\n",
      "On data 3/3193\n",
      "On data 4/3193\n",
      "On data 5/3193\n",
      "On data 6/3193\n",
      "On data 7/3193\n",
      "On data 8/3193\n",
      "On data 9/3193\n",
      "On data 10/3193\n",
      "On data 11/3193\n",
      "On data 12/3193\n",
      "On data 13/3193\n",
      "On data 14/3193\n",
      "On data 15/3193\n",
      "On data 16/3193\n",
      "On data 17/3193\n",
      "On data 18/3193\n",
      "On data 19/3193\n",
      "On data 20/3193\n",
      "On data 21/3193\n",
      "On data 22/3193\n",
      "On data 23/3193\n",
      "On data 24/3193\n",
      "On data 25/3193\n",
      "On data 26/3193\n",
      "On data 27/3193\n",
      "On data 28/3193\n",
      "On data 29/3193\n",
      "On data 30/3193\n",
      "On data 31/3193\n",
      "On data 32/3193\n",
      "On data 33/3193\n",
      "On data 34/3193\n",
      "On data 35/3193\n",
      "On data 36/3193\n",
      "On data 37/3193\n",
      "On data 38/3193\n",
      "On data 39/3193\n",
      "On data 40/3193\n",
      "On data 41/3193\n",
      "On data 42/3193\n",
      "On data 43/3193\n",
      "On data 44/3193\n",
      "On data 45/3193\n",
      "On data 46/3193\n",
      "On data 47/3193\n",
      "On data 48/3193\n",
      "On data 49/3193\n",
      "On data 50/3193\n",
      "On data 51/3193\n",
      "On data 52/3193\n",
      "On data 53/3193\n",
      "On data 54/3193\n",
      "On data 55/3193\n",
      "On data 56/3193\n",
      "On data 57/3193\n",
      "On data 58/3193\n",
      "On data 59/3193\n",
      "On data 60/3193\n",
      "On data 61/3193\n",
      "On data 62/3193\n",
      "On data 63/3193\n",
      "On data 64/3193\n",
      "On data 65/3193\n",
      "On data 66/3193\n",
      "On data 67/3193\n",
      "On data 68/3193\n",
      "On data 69/3193\n",
      "On data 70/3193\n",
      "On data 71/3193\n",
      "On data 72/3193\n",
      "On data 73/3193\n",
      "On data 74/3193\n",
      "On data 75/3193\n",
      "On data 76/3193\n",
      "On data 77/3193\n",
      "On data 78/3193\n",
      "On data 79/3193\n",
      "On data 80/3193\n",
      "On data 81/3193\n",
      "On data 82/3193\n",
      "On data 83/3193\n",
      "On data 84/3193\n",
      "On data 85/3193\n",
      "On data 86/3193\n",
      "On data 87/3193\n",
      "On data 88/3193\n",
      "On data 89/3193\n",
      "On data 90/3193\n",
      "On data 91/3193\n",
      "On data 92/3193\n",
      "On data 93/3193\n",
      "On data 94/3193\n",
      "On data 95/3193\n",
      "On data 96/3193\n",
      "On data 97/3193\n",
      "On data 98/3193\n",
      "On data 99/3193\n",
      "On data 100/3193\n",
      "On data 101/3193\n",
      "On data 102/3193\n",
      "On data 103/3193\n",
      "On data 104/3193\n",
      "On data 105/3193\n",
      "On data 106/3193\n",
      "On data 107/3193\n",
      "On data 108/3193\n",
      "On data 109/3193\n",
      "On data 110/3193\n",
      "On data 111/3193\n",
      "On data 112/3193\n",
      "On data 113/3193\n",
      "On data 114/3193\n",
      "On data 115/3193\n",
      "On data 116/3193\n",
      "On data 117/3193\n",
      "On data 118/3193\n",
      "On data 119/3193\n",
      "On data 120/3193\n",
      "On data 121/3193\n",
      "On data 122/3193\n",
      "On data 123/3193\n",
      "On data 124/3193\n",
      "On data 125/3193\n",
      "On data 126/3193\n",
      "On data 127/3193\n",
      "On data 128/3193\n",
      "On data 129/3193\n",
      "On data 130/3193\n",
      "On data 131/3193\n",
      "On data 132/3193\n",
      "On data 133/3193\n",
      "On data 134/3193\n",
      "On data 135/3193\n",
      "On data 136/3193\n",
      "On data 137/3193\n",
      "On data 138/3193\n",
      "On data 139/3193\n",
      "On data 140/3193\n",
      "On data 141/3193\n",
      "On data 142/3193\n",
      "On data 143/3193\n",
      "On data 144/3193\n",
      "On data 145/3193\n",
      "On data 146/3193\n",
      "On data 147/3193\n",
      "On data 148/3193\n",
      "On data 149/3193\n",
      "On data 150/3193\n",
      "On data 151/3193\n",
      "On data 152/3193\n",
      "On data 153/3193\n",
      "On data 154/3193\n",
      "On data 155/3193\n",
      "On data 156/3193\n",
      "On data 157/3193\n",
      "On data 158/3193\n",
      "On data 159/3193\n",
      "On data 160/3193\n",
      "On data 161/3193\n",
      "On data 162/3193\n",
      "Valid | Loss:218.50400 Acc:0.592\n",
      "epoch running time:1756s\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "training(model=model,\n",
    "         epoch= 4, # TODO: # 40,\n",
    "         train=train_loader,\n",
    "         valid=valid_loader,\n",
    "         device=device)\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out attention mask stuff\n",
    "q = all_quotes[0]\n",
    "encoded_dict = tokenizer.encode_plus(q,\n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=80,\n",
    "                                    padding='max_length',  # instead of pad_to_max_length=True\n",
    "                                    truncation=True,\n",
    "                                    return_tensors='pt',\n",
    "                                    return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1005,  2498, 16958,  2004,  2204,  2004, 15629,  5683,  1012,\n",
       "          1005,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make quotes to bert tensor\n",
    "def make_tensors(quotes):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for q in quotes:\n",
    "        encoded_dict = tokenizer.encode_plus(q,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=80,\n",
    "                                             padding='max_length',  # instead of pad_to_max_length=True\n",
    "                                             truncation=True,\n",
    "                                             return_tensors='pt',\n",
    "                                             return_attention_mask=True)\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, 0)\n",
    "    attention_masks = torch.cat(attention_masks, 0)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quote bert input:\n",
      "torch.Size([6108, 80])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "quote_input_ids, quote_attention_masks = make_tensors(all_quotes)\n",
    "print(\"quote bert input:\")\n",
    "print(quote_input_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_attention_masks[0].unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate sentence vector for quotes\n",
    "quote_model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model_dict = quote_model.state_dict()\n",
    "save_model_state = torch.load(\"./model/english_quote.pth\")\n",
    "state_dict = {k[11:]: v for k, v in save_model_state.items() if k[11:] in model_dict.keys()}\n",
    "model_dict.update(state_dict)\n",
    "quote_model.load_state_dict(model_dict)\n",
    "quote_model = quote_model.to(device)\n",
    "quote_input_ids = quote_input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quote tensor:\n",
      "torch.Size([6108, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "quote_embeddings = []\n",
    "quote_model.eval()\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for input_ids in quote_input_ids:\n",
    "        i += 1\n",
    "        input_ids = input_ids.unsqueeze(dim=0)\n",
    "        # TODO: check if passing in attention mask was the right move\n",
    "        attention_mask = quote_attention_masks[i-1].unsqueeze(dim=0)\n",
    "        \n",
    "        outputs = quote_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs[0]  # hidden_states:[batch_size, sequence_length, hidden_size]\n",
    "        quote_tensor = torch.mean(hidden_states, dim=1)  # quote_tensor: [batch_size, hidden_size]\n",
    "        quote_embeddings.append(quote_tensor)\n",
    "    quote_embeddings = torch.cat(quote_embeddings, dim=0)\n",
    "print(\"quote tensor:\")\n",
    "print(quote_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the mask method for training\n",
    "class QuotRecNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_masks,\n",
    "                mask_ids, quote_tensor):\n",
    "        outputs = self.bert_model(input_ids=input_ids,\n",
    "                                  token_type_ids=token_type_ids,\n",
    "                                  attention_mask=attention_masks)\n",
    "        last_hidden_state = outputs[0]  # [batch_size, sequence_length, hidden_size]\n",
    "        all_outputs = []\n",
    "        for i in range(len(last_hidden_state)):\n",
    "            hidden_state = last_hidden_state[i]  # [sequence_length, hidden_size]\n",
    "            mask = hidden_state[mask_ids[i]]\n",
    "            context = self.dropout(mask)\n",
    "            context = context.unsqueeze(dim=0)  # context: [1, hidden_size]\n",
    "            # quote_tensor: [num_class, hidden_size]\n",
    "            output = torch.mm(context, quote_tensor.t())  # outputs: [1, num_class]\n",
    "            all_outputs.append(output)\n",
    "        all_outputs = torch.cat(all_outputs, dim=0)  # all_outputs: [batch, num_class]\n",
    "        return all_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model......\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuotRecNet(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"loading model......\")\n",
    "model = QuotRecNet()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get rank\n",
    "def rank_gold(predicts, golds):\n",
    "    ranks = []\n",
    "    ps = predicts.data.cpu().numpy()\n",
    "    gs = golds.cpu().numpy()\n",
    "    for i in range(len(ps)):\n",
    "        predict = ps[i]\n",
    "        gold_index = gs[i]\n",
    "        predict_value = predict[gold_index]\n",
    "        predict_sort = sorted(predict, reverse=True)\n",
    "        predict_index = predict_sort.index(predict_value)\n",
    "        if predict_index == -1:\n",
    "            break\n",
    "        ranks.append(predict_index)\n",
    "    return ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get NDCG@5\n",
    "def get_NDCG(ranks):\n",
    "    total = 0.0\n",
    "    for r in ranks:\n",
    "        if r < 5:  # k=5\n",
    "            total += 1.0 / np.log2(r + 2)\n",
    "    return total / len(ranks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get recall@k\n",
    "def recall(predicts, golds):\n",
    "    predicts = predicts.data.cpu().numpy()\n",
    "    golds = golds.cpu().numpy()\n",
    "    predicts_index = np.argsort(-predicts, axis=1)\n",
    "    recall_1, recall_3, recall_5, recall_10, recall_20, recall_30 = 0, 0, 0, 0, 0, 0\n",
    "    recall_100, recall_300, recall_500, recall_1000 = 0, 0, 0, 0\n",
    "    for i in range(len(golds)):\n",
    "        if golds[i] in predicts_index[i][:1000]:\n",
    "            recall_1000 += 1\n",
    "            if golds[i] in predicts_index[i][:500]:\n",
    "                recall_500 += 1\n",
    "                if golds[i] in predicts_index[i][:300]:\n",
    "                    recall_300 += 1\n",
    "                    if golds[i] in predicts_index[i][:100]:\n",
    "                        recall_100 += 1\n",
    "                        if golds[i] in predicts_index[i][:30]:\n",
    "                            recall_30 += 1\n",
    "                            if golds[i] in predicts_index[i][:20]:\n",
    "                                recall_20 += 1\n",
    "                                if golds[i] in predicts_index[i][:10]:\n",
    "                                    recall_10 += 1\n",
    "                                    if golds[i] in predicts_index[i][:5]:\n",
    "                                        recall_5 += 1\n",
    "                                        if golds[i] in predicts_index[i][:3]:\n",
    "                                            recall_3 += 1\n",
    "                                            if golds[i] in predicts_index[\n",
    "                                                    i][:1]:\n",
    "                                                recall_1 += 1\n",
    "    return recall_1, recall_3, recall_5, recall_10, recall_20, recall_30, recall_100, recall_300, recall_500, recall_1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_mask(model, epoch, train, valid, quote_tensor, device):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(\n",
    "        total, trainable))\n",
    "    t_batch = len(train)\n",
    "    v_batch = len(valid)\n",
    "\n",
    "    # NOTE: the lengths of the datasets are too long\n",
    "    # in training takes ~14 sec for each data point\n",
    "    # say i want max 0.5 hr\n",
    "    max_num_data = int(0.5*60*60/14)\n",
    "    print(f\"max # data: {max_num_data}\")\n",
    "    if t_batch > max_num_data:\n",
    "        # train = train[0:max_num_data]\n",
    "        t_batch = max_num_data\n",
    "    \n",
    "    if v_batch > max_num_data:\n",
    "        # valid = valid[0:max_num_data]\n",
    "        v_batch = max_num_data\n",
    "\n",
    "    learning_rate = 5e-5\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    best_MRR = 0\n",
    "    count = 0\n",
    "    quote_tensor = quote_tensor.to(device)\n",
    "    for epoch in range(epoch):\n",
    "        start = time.perf_counter()\n",
    "        print(\"epoch: \", epoch + 1)\n",
    "        total_loss, total_MRR, total_NDCG = 0, 0, 0\n",
    "        # train\n",
    "        for i, (input_ids, token_type_ids, attention_masks, mask_ids, labels) in enumerate(train):\n",
    "            if i >= t_batch:\n",
    "                break\n",
    "\n",
    "            print(f\"On data {i}/{len(train)}\")\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            mask_ids = mask_ids.to(device, dtype=torch.long)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, token_type_ids, attention_masks, mask_ids, quote_tensor)  # outputs: (batch, num_class)\n",
    "            # print(\"outputs:\", outputs.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ranks = rank_gold(outputs, labels)\n",
    "            MRR = np.average([1.0 / (r + 1) for r in ranks])\n",
    "            NDCG = get_NDCG(ranks)\n",
    "            total_loss += loss.item()\n",
    "            total_MRR += MRR\n",
    "            total_NDCG += NDCG\n",
    "        end = time.perf_counter()\n",
    "        print('Epoch running time :{:.0f}'.format(end - start))\n",
    "        print('Train | Loss:{:.3f} MRR: {:.3f} NDCG: {:.3f}'.format(total_loss, total_MRR/t_batch, total_NDCG/t_batch))\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_MRR, total_NDCG = 0, 0, 0\n",
    "            for i, (input_ids, token_type_ids, attention_masks, mask_ids, labels) in enumerate(valid):\n",
    "                if i >= v_batch:\n",
    "                    break\n",
    "\n",
    "                print(f\"On data {i}/{len(valid)}\")\n",
    "\n",
    "                input_ids = input_ids.to(device)\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "                attention_masks = attention_masks.to(device)\n",
    "                mask_ids = mask_ids.to(device, dtype=torch.long)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                outputs = model(input_ids, token_type_ids, attention_masks, mask_ids, quote_tensor)\n",
    "                loss = criterion(outputs, labels)\n",
    "                ranks = rank_gold(outputs, labels)\n",
    "                MRR = np.average([1.0 / (r + 1) for r in ranks])\n",
    "                NDCG = get_NDCG(ranks)\n",
    "                total_loss += loss.item()\n",
    "                total_MRR += MRR\n",
    "                total_NDCG += NDCG\n",
    "            print(\"Valid | Loss:{:.5f} MRR: {:.3f} NDCG: {:.3f}\".format(\n",
    "                total_loss, total_MRR / v_batch,\n",
    "                total_NDCG / v_batch))\n",
    "        if total_MRR > best_MRR:\n",
    "            best_MRR = total_MRR\n",
    "            torch.save(model, \"./model/model_english.model\")\n",
    "            print('saving model with MRR {:.3f} NDCG: {:.3f}'.format(\n",
    "                total_MRR / v_batch, total_NDCG / v_batch))\n",
    "            count = 0\n",
    "        else:\n",
    "            learning_rate = learning_rate * 0.9\n",
    "            count += 1\n",
    "        # early stopping\n",
    "        if count == 3:\n",
    "            break\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mask Dataset and DataLoader\n",
    "class Dataset_Mask(Dataset):\n",
    "\n",
    "    def __init__(self, input_ids, token_type_ids, attention_masks, mask_ids,\n",
    "                 y):\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.mask_ids = mask_ids\n",
    "        self.label = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None:\n",
    "            return self.input_ids[idx], self.token_type_ids[\n",
    "                idx], self.attention_masks[idx], self.mask_ids[idx]\n",
    "        return self.input_ids[idx], self.token_type_ids[\n",
    "            idx], self.attention_masks[idx], self.mask_ids[idx], self.label[\n",
    "                idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train and valid dataloader ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"loading train and valid dataloader ...\")\n",
    "train_dataset_mask = Dataset_Mask(input_ids=train_input_ids,\n",
    "                                  token_type_ids=train_token_type_ids,\n",
    "                                  attention_masks=train_attention_masks,\n",
    "                                  mask_ids=train_mask_ids,\n",
    "                                  y=y_train)\n",
    "train_loader_mask = DataLoader(dataset=train_dataset_mask,\n",
    "                               batch_size=64,\n",
    "                               shuffle=True,\n",
    "                               num_workers= 0) # 2)\n",
    "valid_dataset_mask = Dataset_Mask(input_ids=valid_input_ids,\n",
    "                                  token_type_ids=valid_token_type_ids,\n",
    "                                  attention_masks=valid_attention_masks,\n",
    "                                  mask_ids=valid_mask_ids,\n",
    "                                  y=y_valid)\n",
    "valid_loader_mask = DataLoader(dataset=valid_dataset_mask,\n",
    "                               batch_size=64,\n",
    "                               shuffle=True,\n",
    "                               num_workers=0)# 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start traing......\n",
      "\n",
      "start training, parameter total:109482240, trainable:109482240\n",
      "\n",
      "max # data: 128\n",
      "epoch:  1\n",
      "On data 0/1581\n",
      "On data 1/1581\n",
      "On data 2/1581\n",
      "On data 3/1581\n",
      "On data 4/1581\n",
      "On data 5/1581\n",
      "On data 6/1581\n",
      "On data 7/1581\n",
      "On data 8/1581\n",
      "On data 9/1581\n",
      "On data 10/1581\n",
      "On data 11/1581\n",
      "On data 12/1581\n",
      "On data 13/1581\n",
      "On data 14/1581\n",
      "On data 15/1581\n",
      "On data 16/1581\n",
      "On data 17/1581\n",
      "On data 18/1581\n",
      "On data 19/1581\n",
      "On data 20/1581\n",
      "On data 21/1581\n",
      "On data 22/1581\n",
      "On data 23/1581\n",
      "On data 24/1581\n",
      "On data 25/1581\n",
      "On data 26/1581\n",
      "On data 27/1581\n",
      "On data 28/1581\n",
      "On data 29/1581\n",
      "On data 30/1581\n",
      "On data 31/1581\n",
      "On data 32/1581\n",
      "On data 33/1581\n",
      "On data 34/1581\n",
      "On data 35/1581\n",
      "On data 36/1581\n",
      "On data 37/1581\n",
      "On data 38/1581\n",
      "On data 39/1581\n",
      "On data 40/1581\n",
      "On data 41/1581\n",
      "On data 42/1581\n",
      "On data 43/1581\n",
      "On data 44/1581\n",
      "On data 45/1581\n",
      "On data 46/1581\n",
      "On data 47/1581\n",
      "On data 48/1581\n",
      "On data 49/1581\n",
      "On data 50/1581\n",
      "On data 51/1581\n",
      "On data 52/1581\n",
      "On data 53/1581\n",
      "On data 54/1581\n",
      "On data 55/1581\n",
      "On data 56/1581\n",
      "On data 57/1581\n",
      "On data 58/1581\n",
      "On data 59/1581\n",
      "On data 60/1581\n",
      "On data 61/1581\n",
      "On data 62/1581\n",
      "On data 63/1581\n",
      "On data 64/1581\n",
      "On data 65/1581\n",
      "On data 66/1581\n",
      "On data 67/1581\n",
      "On data 68/1581\n",
      "On data 69/1581\n",
      "On data 70/1581\n",
      "On data 71/1581\n",
      "On data 72/1581\n",
      "On data 73/1581\n",
      "On data 74/1581\n",
      "On data 75/1581\n",
      "On data 76/1581\n",
      "On data 77/1581\n",
      "On data 78/1581\n",
      "On data 79/1581\n",
      "On data 80/1581\n",
      "On data 81/1581\n",
      "On data 82/1581\n",
      "On data 83/1581\n",
      "On data 84/1581\n",
      "On data 85/1581\n",
      "On data 86/1581\n",
      "On data 87/1581\n",
      "On data 88/1581\n",
      "On data 89/1581\n",
      "On data 90/1581\n",
      "On data 91/1581\n",
      "On data 92/1581\n",
      "On data 93/1581\n",
      "On data 94/1581\n",
      "On data 95/1581\n",
      "On data 96/1581\n",
      "On data 97/1581\n",
      "On data 98/1581\n",
      "On data 99/1581\n",
      "On data 100/1581\n",
      "On data 101/1581\n",
      "On data 102/1581\n",
      "On data 103/1581\n",
      "On data 104/1581\n",
      "On data 105/1581\n",
      "On data 106/1581\n",
      "On data 107/1581\n",
      "On data 108/1581\n",
      "On data 109/1581\n",
      "On data 110/1581\n",
      "On data 111/1581\n",
      "On data 112/1581\n",
      "On data 113/1581\n",
      "On data 114/1581\n",
      "On data 115/1581\n",
      "On data 116/1581\n",
      "On data 117/1581\n",
      "On data 118/1581\n",
      "On data 119/1581\n",
      "On data 120/1581\n",
      "On data 121/1581\n",
      "On data 122/1581\n",
      "On data 123/1581\n",
      "On data 124/1581\n",
      "On data 125/1581\n",
      "On data 126/1581\n",
      "On data 127/1581\n",
      "Epoch running time :1669\n",
      "Train | Loss:778.222 MRR: 0.163 NDCG: 0.163\n",
      "On data 0/200\n",
      "On data 1/200\n",
      "On data 2/200\n",
      "On data 3/200\n",
      "On data 4/200\n",
      "On data 5/200\n",
      "On data 6/200\n",
      "On data 7/200\n",
      "On data 8/200\n",
      "On data 9/200\n",
      "On data 10/200\n",
      "On data 11/200\n",
      "On data 12/200\n",
      "On data 13/200\n",
      "On data 14/200\n",
      "On data 15/200\n",
      "On data 16/200\n",
      "On data 17/200\n",
      "On data 18/200\n",
      "On data 19/200\n",
      "On data 20/200\n",
      "On data 21/200\n",
      "On data 22/200\n",
      "On data 23/200\n",
      "On data 24/200\n",
      "On data 25/200\n",
      "On data 26/200\n",
      "On data 27/200\n",
      "On data 28/200\n",
      "On data 29/200\n",
      "On data 30/200\n",
      "On data 31/200\n",
      "On data 32/200\n",
      "On data 33/200\n",
      "On data 34/200\n",
      "On data 35/200\n",
      "On data 36/200\n",
      "On data 37/200\n",
      "On data 38/200\n",
      "On data 39/200\n",
      "On data 40/200\n",
      "On data 41/200\n",
      "On data 42/200\n",
      "On data 43/200\n",
      "On data 44/200\n",
      "On data 45/200\n",
      "On data 46/200\n",
      "On data 47/200\n",
      "On data 48/200\n",
      "On data 49/200\n",
      "On data 50/200\n",
      "On data 51/200\n",
      "On data 52/200\n",
      "On data 53/200\n",
      "On data 54/200\n",
      "On data 55/200\n",
      "On data 56/200\n",
      "On data 57/200\n",
      "On data 58/200\n",
      "On data 59/200\n",
      "On data 60/200\n",
      "On data 61/200\n",
      "On data 62/200\n",
      "On data 63/200\n",
      "On data 64/200\n",
      "On data 65/200\n",
      "On data 66/200\n",
      "On data 67/200\n",
      "On data 68/200\n",
      "On data 69/200\n",
      "On data 70/200\n",
      "On data 71/200\n",
      "On data 72/200\n",
      "On data 73/200\n",
      "On data 74/200\n",
      "On data 75/200\n",
      "On data 76/200\n",
      "On data 77/200\n",
      "On data 78/200\n",
      "On data 79/200\n",
      "On data 80/200\n",
      "On data 81/200\n",
      "On data 82/200\n",
      "On data 83/200\n",
      "On data 84/200\n",
      "On data 85/200\n",
      "On data 86/200\n",
      "On data 87/200\n",
      "On data 88/200\n",
      "On data 89/200\n",
      "On data 90/200\n",
      "On data 91/200\n",
      "On data 92/200\n",
      "On data 93/200\n",
      "On data 94/200\n",
      "On data 95/200\n",
      "On data 96/200\n",
      "On data 97/200\n",
      "On data 98/200\n",
      "On data 99/200\n",
      "On data 100/200\n",
      "On data 101/200\n",
      "On data 102/200\n",
      "On data 103/200\n",
      "On data 104/200\n",
      "On data 105/200\n",
      "On data 106/200\n",
      "On data 107/200\n",
      "On data 108/200\n",
      "On data 109/200\n",
      "On data 110/200\n",
      "On data 111/200\n",
      "On data 112/200\n",
      "On data 113/200\n",
      "On data 114/200\n",
      "On data 115/200\n",
      "On data 116/200\n",
      "On data 117/200\n",
      "On data 118/200\n",
      "On data 119/200\n",
      "On data 120/200\n",
      "On data 121/200\n",
      "On data 122/200\n",
      "On data 123/200\n",
      "On data 124/200\n",
      "On data 125/200\n",
      "On data 126/200\n",
      "On data 127/200\n",
      "Valid | Loss:741.91341 MRR: 0.192 NDCG: 0.192\n",
      "saving model with MRR 0.192 NDCG: 0.192\n",
      "epoch:  2\n",
      "On data 0/1581\n",
      "On data 1/1581\n",
      "On data 2/1581\n",
      "On data 3/1581\n",
      "On data 4/1581\n",
      "On data 5/1581\n",
      "On data 6/1581\n",
      "On data 7/1581\n",
      "On data 8/1581\n",
      "On data 9/1581\n",
      "On data 10/1581\n",
      "On data 11/1581\n",
      "On data 12/1581\n",
      "On data 13/1581\n",
      "On data 14/1581\n",
      "On data 15/1581\n",
      "On data 16/1581\n",
      "On data 17/1581\n",
      "On data 18/1581\n",
      "On data 19/1581\n",
      "On data 20/1581\n",
      "On data 21/1581\n",
      "On data 22/1581\n",
      "On data 23/1581\n",
      "On data 24/1581\n",
      "On data 25/1581\n",
      "On data 26/1581\n",
      "On data 27/1581\n",
      "On data 28/1581\n",
      "On data 29/1581\n",
      "On data 30/1581\n",
      "On data 31/1581\n",
      "On data 32/1581\n",
      "On data 33/1581\n",
      "On data 34/1581\n",
      "On data 35/1581\n",
      "On data 36/1581\n",
      "On data 37/1581\n",
      "On data 38/1581\n",
      "On data 39/1581\n",
      "On data 40/1581\n",
      "On data 41/1581\n",
      "On data 42/1581\n",
      "On data 43/1581\n",
      "On data 44/1581\n",
      "On data 45/1581\n",
      "On data 46/1581\n",
      "On data 47/1581\n",
      "On data 48/1581\n",
      "On data 49/1581\n",
      "On data 50/1581\n",
      "On data 51/1581\n",
      "On data 52/1581\n",
      "On data 53/1581\n",
      "On data 54/1581\n",
      "On data 55/1581\n",
      "On data 56/1581\n",
      "On data 57/1581\n",
      "On data 58/1581\n",
      "On data 59/1581\n",
      "On data 60/1581\n",
      "On data 61/1581\n",
      "On data 62/1581\n",
      "On data 63/1581\n",
      "On data 64/1581\n",
      "On data 65/1581\n",
      "On data 66/1581\n",
      "On data 67/1581\n",
      "On data 68/1581\n",
      "On data 69/1581\n",
      "On data 70/1581\n",
      "On data 71/1581\n",
      "On data 72/1581\n",
      "On data 73/1581\n",
      "On data 74/1581\n",
      "On data 75/1581\n",
      "On data 76/1581\n",
      "On data 77/1581\n",
      "On data 78/1581\n",
      "On data 79/1581\n",
      "On data 80/1581\n",
      "On data 81/1581\n",
      "On data 82/1581\n",
      "On data 83/1581\n",
      "On data 84/1581\n",
      "On data 85/1581\n",
      "On data 86/1581\n",
      "On data 87/1581\n",
      "On data 88/1581\n",
      "On data 89/1581\n",
      "On data 90/1581\n",
      "On data 91/1581\n",
      "On data 92/1581\n",
      "On data 93/1581\n",
      "On data 94/1581\n",
      "On data 95/1581\n",
      "On data 96/1581\n",
      "On data 97/1581\n",
      "On data 98/1581\n",
      "On data 99/1581\n",
      "On data 100/1581\n",
      "On data 101/1581\n",
      "On data 102/1581\n",
      "On data 103/1581\n",
      "On data 104/1581\n",
      "On data 105/1581\n",
      "On data 106/1581\n",
      "On data 107/1581\n",
      "On data 108/1581\n",
      "On data 109/1581\n",
      "On data 110/1581\n",
      "On data 111/1581\n",
      "On data 112/1581\n",
      "On data 113/1581\n",
      "On data 114/1581\n",
      "On data 115/1581\n",
      "On data 116/1581\n",
      "On data 117/1581\n",
      "On data 118/1581\n",
      "On data 119/1581\n",
      "On data 120/1581\n",
      "On data 121/1581\n",
      "On data 122/1581\n",
      "On data 123/1581\n",
      "On data 124/1581\n",
      "On data 125/1581\n",
      "On data 126/1581\n",
      "On data 127/1581\n",
      "Epoch running time :1604\n",
      "Train | Loss:708.854 MRR: 0.220 NDCG: 0.222\n",
      "On data 0/200\n",
      "On data 1/200\n",
      "On data 2/200\n",
      "On data 3/200\n",
      "On data 4/200\n",
      "On data 5/200\n",
      "On data 6/200\n",
      "On data 7/200\n",
      "On data 8/200\n",
      "On data 9/200\n",
      "On data 10/200\n",
      "On data 11/200\n",
      "On data 12/200\n",
      "On data 13/200\n",
      "On data 14/200\n",
      "On data 15/200\n",
      "On data 16/200\n",
      "On data 17/200\n",
      "On data 18/200\n",
      "On data 19/200\n",
      "On data 20/200\n",
      "On data 21/200\n",
      "On data 22/200\n",
      "On data 23/200\n",
      "On data 24/200\n",
      "On data 25/200\n",
      "On data 26/200\n",
      "On data 27/200\n",
      "On data 28/200\n",
      "On data 29/200\n",
      "On data 30/200\n",
      "On data 31/200\n",
      "On data 32/200\n",
      "On data 33/200\n",
      "On data 34/200\n",
      "On data 35/200\n",
      "On data 36/200\n",
      "On data 37/200\n",
      "On data 38/200\n",
      "On data 39/200\n",
      "On data 40/200\n",
      "On data 41/200\n",
      "On data 42/200\n",
      "On data 43/200\n",
      "On data 44/200\n",
      "On data 45/200\n",
      "On data 46/200\n",
      "On data 47/200\n",
      "On data 48/200\n",
      "On data 49/200\n",
      "On data 50/200\n",
      "On data 51/200\n",
      "On data 52/200\n",
      "On data 53/200\n",
      "On data 54/200\n",
      "On data 55/200\n",
      "On data 56/200\n",
      "On data 57/200\n",
      "On data 58/200\n",
      "On data 59/200\n",
      "On data 60/200\n",
      "On data 61/200\n",
      "On data 62/200\n",
      "On data 63/200\n",
      "On data 64/200\n",
      "On data 65/200\n",
      "On data 66/200\n",
      "On data 67/200\n",
      "On data 68/200\n",
      "On data 69/200\n",
      "On data 70/200\n",
      "On data 71/200\n",
      "On data 72/200\n",
      "On data 73/200\n",
      "On data 74/200\n",
      "On data 75/200\n",
      "On data 76/200\n",
      "On data 77/200\n",
      "On data 78/200\n",
      "On data 79/200\n",
      "On data 80/200\n",
      "On data 81/200\n",
      "On data 82/200\n",
      "On data 83/200\n",
      "On data 84/200\n",
      "On data 85/200\n",
      "On data 86/200\n",
      "On data 87/200\n",
      "On data 88/200\n",
      "On data 89/200\n",
      "On data 90/200\n",
      "On data 91/200\n",
      "On data 92/200\n",
      "On data 93/200\n",
      "On data 94/200\n",
      "On data 95/200\n",
      "On data 96/200\n",
      "On data 97/200\n",
      "On data 98/200\n",
      "On data 99/200\n",
      "On data 100/200\n",
      "On data 101/200\n",
      "On data 102/200\n",
      "On data 103/200\n",
      "On data 104/200\n",
      "On data 105/200\n",
      "On data 106/200\n",
      "On data 107/200\n",
      "On data 108/200\n",
      "On data 109/200\n",
      "On data 110/200\n",
      "On data 111/200\n",
      "On data 112/200\n",
      "On data 113/200\n",
      "On data 114/200\n",
      "On data 115/200\n",
      "On data 116/200\n",
      "On data 117/200\n",
      "On data 118/200\n",
      "On data 119/200\n",
      "On data 120/200\n",
      "On data 121/200\n",
      "On data 122/200\n",
      "On data 123/200\n",
      "On data 124/200\n",
      "On data 125/200\n",
      "On data 126/200\n",
      "On data 127/200\n",
      "Valid | Loss:705.97894 MRR: 0.225 NDCG: 0.228\n",
      "saving model with MRR 0.225 NDCG: 0.228\n",
      "epoch:  3\n",
      "On data 0/1581\n",
      "On data 1/1581\n",
      "On data 2/1581\n",
      "On data 3/1581\n",
      "On data 4/1581\n",
      "On data 5/1581\n",
      "On data 6/1581\n",
      "On data 7/1581\n",
      "On data 8/1581\n",
      "On data 9/1581\n",
      "On data 10/1581\n",
      "On data 11/1581\n",
      "On data 12/1581\n",
      "On data 13/1581\n",
      "On data 14/1581\n",
      "On data 15/1581\n",
      "On data 16/1581\n",
      "On data 17/1581\n",
      "On data 18/1581\n",
      "On data 19/1581\n",
      "On data 20/1581\n",
      "On data 21/1581\n",
      "On data 22/1581\n",
      "On data 23/1581\n",
      "On data 24/1581\n",
      "On data 25/1581\n",
      "On data 26/1581\n",
      "On data 27/1581\n",
      "On data 28/1581\n",
      "On data 29/1581\n",
      "On data 30/1581\n",
      "On data 31/1581\n",
      "On data 32/1581\n",
      "On data 33/1581\n",
      "On data 34/1581\n",
      "On data 35/1581\n",
      "On data 36/1581\n",
      "On data 37/1581\n",
      "On data 38/1581\n",
      "On data 39/1581\n",
      "On data 40/1581\n",
      "On data 41/1581\n",
      "On data 42/1581\n",
      "On data 43/1581\n",
      "On data 44/1581\n",
      "On data 45/1581\n",
      "On data 46/1581\n",
      "On data 47/1581\n",
      "On data 48/1581\n",
      "On data 49/1581\n",
      "On data 50/1581\n",
      "On data 51/1581\n",
      "On data 52/1581\n",
      "On data 53/1581\n",
      "On data 54/1581\n",
      "On data 55/1581\n",
      "On data 56/1581\n",
      "On data 57/1581\n",
      "On data 58/1581\n",
      "On data 59/1581\n",
      "On data 60/1581\n",
      "On data 61/1581\n",
      "On data 62/1581\n",
      "On data 63/1581\n",
      "On data 64/1581\n",
      "On data 65/1581\n",
      "On data 66/1581\n",
      "On data 67/1581\n",
      "On data 68/1581\n",
      "On data 69/1581\n",
      "On data 70/1581\n",
      "On data 71/1581\n",
      "On data 72/1581\n",
      "On data 73/1581\n",
      "On data 74/1581\n",
      "On data 75/1581\n",
      "On data 76/1581\n",
      "On data 77/1581\n",
      "On data 78/1581\n",
      "On data 79/1581\n",
      "On data 80/1581\n",
      "On data 81/1581\n",
      "On data 82/1581\n",
      "On data 83/1581\n",
      "On data 84/1581\n",
      "On data 85/1581\n",
      "On data 86/1581\n",
      "On data 87/1581\n",
      "On data 88/1581\n",
      "On data 89/1581\n",
      "On data 90/1581\n",
      "On data 91/1581\n",
      "On data 92/1581\n",
      "On data 93/1581\n",
      "On data 94/1581\n",
      "On data 95/1581\n",
      "On data 96/1581\n",
      "On data 97/1581\n",
      "On data 98/1581\n",
      "On data 99/1581\n",
      "On data 100/1581\n",
      "On data 101/1581\n",
      "On data 102/1581\n",
      "On data 103/1581\n",
      "On data 104/1581\n",
      "On data 105/1581\n",
      "On data 106/1581\n",
      "On data 107/1581\n",
      "On data 108/1581\n",
      "On data 109/1581\n",
      "On data 110/1581\n",
      "On data 111/1581\n",
      "On data 112/1581\n",
      "On data 113/1581\n",
      "On data 114/1581\n",
      "On data 115/1581\n",
      "On data 116/1581\n",
      "On data 117/1581\n",
      "On data 118/1581\n",
      "On data 119/1581\n",
      "On data 120/1581\n",
      "On data 121/1581\n",
      "On data 122/1581\n",
      "On data 123/1581\n",
      "On data 124/1581\n",
      "On data 125/1581\n",
      "On data 126/1581\n",
      "On data 127/1581\n",
      "Epoch running time :1632\n",
      "Train | Loss:670.135 MRR: 0.262 NDCG: 0.267\n",
      "On data 0/200\n",
      "On data 1/200\n",
      "On data 2/200\n",
      "On data 3/200\n",
      "On data 4/200\n",
      "On data 5/200\n",
      "On data 6/200\n",
      "On data 7/200\n",
      "On data 8/200\n",
      "On data 9/200\n",
      "On data 10/200\n",
      "On data 11/200\n",
      "On data 12/200\n",
      "On data 13/200\n",
      "On data 14/200\n",
      "On data 15/200\n",
      "On data 16/200\n",
      "On data 17/200\n",
      "On data 18/200\n",
      "On data 19/200\n",
      "On data 20/200\n",
      "On data 21/200\n",
      "On data 22/200\n",
      "On data 23/200\n",
      "On data 24/200\n",
      "On data 25/200\n",
      "On data 26/200\n",
      "On data 27/200\n",
      "On data 28/200\n",
      "On data 29/200\n",
      "On data 30/200\n",
      "On data 31/200\n",
      "On data 32/200\n",
      "On data 33/200\n",
      "On data 34/200\n",
      "On data 35/200\n",
      "On data 36/200\n",
      "On data 37/200\n",
      "On data 38/200\n",
      "On data 39/200\n",
      "On data 40/200\n",
      "On data 41/200\n",
      "On data 42/200\n",
      "On data 43/200\n",
      "On data 44/200\n",
      "On data 45/200\n",
      "On data 46/200\n",
      "On data 47/200\n",
      "On data 48/200\n",
      "On data 49/200\n",
      "On data 50/200\n",
      "On data 51/200\n",
      "On data 52/200\n",
      "On data 53/200\n",
      "On data 54/200\n",
      "On data 55/200\n",
      "On data 56/200\n",
      "On data 57/200\n",
      "On data 58/200\n",
      "On data 59/200\n",
      "On data 60/200\n",
      "On data 61/200\n",
      "On data 62/200\n",
      "On data 63/200\n",
      "On data 64/200\n",
      "On data 65/200\n",
      "On data 66/200\n",
      "On data 67/200\n",
      "On data 68/200\n",
      "On data 69/200\n",
      "On data 70/200\n",
      "On data 71/200\n",
      "On data 72/200\n",
      "On data 73/200\n",
      "On data 74/200\n",
      "On data 75/200\n",
      "On data 76/200\n",
      "On data 77/200\n",
      "On data 78/200\n",
      "On data 79/200\n",
      "On data 80/200\n",
      "On data 81/200\n",
      "On data 82/200\n",
      "On data 83/200\n",
      "On data 84/200\n",
      "On data 85/200\n",
      "On data 86/200\n",
      "On data 87/200\n",
      "On data 88/200\n",
      "On data 89/200\n",
      "On data 90/200\n",
      "On data 91/200\n",
      "On data 92/200\n",
      "On data 93/200\n",
      "On data 94/200\n",
      "On data 95/200\n",
      "On data 96/200\n",
      "On data 97/200\n",
      "On data 98/200\n",
      "On data 99/200\n",
      "On data 100/200\n",
      "On data 101/200\n",
      "On data 102/200\n",
      "On data 103/200\n",
      "On data 104/200\n",
      "On data 105/200\n",
      "On data 106/200\n",
      "On data 107/200\n",
      "On data 108/200\n",
      "On data 109/200\n",
      "On data 110/200\n",
      "On data 111/200\n",
      "On data 112/200\n",
      "On data 113/200\n",
      "On data 114/200\n",
      "On data 115/200\n",
      "On data 116/200\n",
      "On data 117/200\n",
      "On data 118/200\n",
      "On data 119/200\n",
      "On data 120/200\n",
      "On data 121/200\n",
      "On data 122/200\n",
      "On data 123/200\n",
      "On data 124/200\n",
      "On data 125/200\n",
      "On data 126/200\n",
      "On data 127/200\n",
      "Valid | Loss:677.19277 MRR: 0.254 NDCG: 0.258\n",
      "saving model with MRR 0.254 NDCG: 0.258\n",
      "epoch:  4\n",
      "On data 0/1581\n",
      "On data 1/1581\n",
      "On data 2/1581\n",
      "On data 3/1581\n",
      "On data 4/1581\n",
      "On data 5/1581\n",
      "On data 6/1581\n",
      "On data 7/1581\n",
      "On data 8/1581\n",
      "On data 9/1581\n",
      "On data 10/1581\n",
      "On data 11/1581\n",
      "On data 12/1581\n",
      "On data 13/1581\n",
      "On data 14/1581\n",
      "On data 15/1581\n",
      "On data 16/1581\n",
      "On data 17/1581\n",
      "On data 18/1581\n",
      "On data 19/1581\n",
      "On data 20/1581\n",
      "On data 21/1581\n",
      "On data 22/1581\n",
      "On data 23/1581\n",
      "On data 24/1581\n",
      "On data 25/1581\n",
      "On data 26/1581\n",
      "On data 27/1581\n",
      "On data 28/1581\n",
      "On data 29/1581\n",
      "On data 30/1581\n",
      "On data 31/1581\n",
      "On data 32/1581\n",
      "On data 33/1581\n",
      "On data 34/1581\n",
      "On data 35/1581\n",
      "On data 36/1581\n",
      "On data 37/1581\n",
      "On data 38/1581\n",
      "On data 39/1581\n",
      "On data 40/1581\n",
      "On data 41/1581\n",
      "On data 42/1581\n",
      "On data 43/1581\n",
      "On data 44/1581\n",
      "On data 45/1581\n",
      "On data 46/1581\n",
      "On data 47/1581\n",
      "On data 48/1581\n",
      "On data 49/1581\n",
      "On data 50/1581\n",
      "On data 51/1581\n",
      "On data 52/1581\n",
      "On data 53/1581\n",
      "On data 54/1581\n",
      "On data 55/1581\n",
      "On data 56/1581\n",
      "On data 57/1581\n",
      "On data 58/1581\n",
      "On data 59/1581\n",
      "On data 60/1581\n",
      "On data 61/1581\n",
      "On data 62/1581\n",
      "On data 63/1581\n",
      "On data 64/1581\n",
      "On data 65/1581\n",
      "On data 66/1581\n",
      "On data 67/1581\n",
      "On data 68/1581\n",
      "On data 69/1581\n",
      "On data 70/1581\n",
      "On data 71/1581\n",
      "On data 72/1581\n",
      "On data 73/1581\n",
      "On data 74/1581\n",
      "On data 75/1581\n",
      "On data 76/1581\n",
      "On data 77/1581\n",
      "On data 78/1581\n",
      "On data 79/1581\n",
      "On data 80/1581\n",
      "On data 81/1581\n",
      "On data 82/1581\n",
      "On data 83/1581\n",
      "On data 84/1581\n",
      "On data 85/1581\n",
      "On data 86/1581\n",
      "On data 87/1581\n",
      "On data 88/1581\n",
      "On data 89/1581\n",
      "On data 90/1581\n",
      "On data 91/1581\n",
      "On data 92/1581\n",
      "On data 93/1581\n",
      "On data 94/1581\n",
      "On data 95/1581\n",
      "On data 96/1581\n",
      "On data 97/1581\n",
      "On data 98/1581\n",
      "On data 99/1581\n",
      "On data 100/1581\n",
      "On data 101/1581\n",
      "On data 102/1581\n",
      "On data 103/1581\n",
      "On data 104/1581\n",
      "On data 105/1581\n",
      "On data 106/1581\n",
      "On data 107/1581\n",
      "On data 108/1581\n",
      "On data 109/1581\n",
      "On data 110/1581\n",
      "On data 111/1581\n",
      "On data 112/1581\n",
      "On data 113/1581\n",
      "On data 114/1581\n",
      "On data 115/1581\n",
      "On data 116/1581\n",
      "On data 117/1581\n",
      "On data 118/1581\n",
      "On data 119/1581\n",
      "On data 120/1581\n",
      "On data 121/1581\n",
      "On data 122/1581\n",
      "On data 123/1581\n",
      "On data 124/1581\n",
      "On data 125/1581\n",
      "On data 126/1581\n",
      "On data 127/1581\n",
      "Epoch running time :1624\n",
      "Train | Loss:627.744 MRR: 0.302 NDCG: 0.308\n",
      "On data 0/200\n",
      "On data 1/200\n",
      "On data 2/200\n",
      "On data 3/200\n",
      "On data 4/200\n",
      "On data 5/200\n",
      "On data 6/200\n",
      "On data 7/200\n",
      "On data 8/200\n",
      "On data 9/200\n",
      "On data 10/200\n",
      "On data 11/200\n",
      "On data 12/200\n",
      "On data 13/200\n",
      "On data 14/200\n",
      "On data 15/200\n",
      "On data 16/200\n",
      "On data 17/200\n",
      "On data 18/200\n",
      "On data 19/200\n",
      "On data 20/200\n",
      "On data 21/200\n",
      "On data 22/200\n",
      "On data 23/200\n",
      "On data 24/200\n",
      "On data 25/200\n",
      "On data 26/200\n",
      "On data 27/200\n",
      "On data 28/200\n",
      "On data 29/200\n",
      "On data 30/200\n",
      "On data 31/200\n",
      "On data 32/200\n",
      "On data 33/200\n",
      "On data 34/200\n",
      "On data 35/200\n",
      "On data 36/200\n",
      "On data 37/200\n",
      "On data 38/200\n",
      "On data 39/200\n",
      "On data 40/200\n",
      "On data 41/200\n",
      "On data 42/200\n",
      "On data 43/200\n",
      "On data 44/200\n",
      "On data 45/200\n",
      "On data 46/200\n",
      "On data 47/200\n",
      "On data 48/200\n",
      "On data 49/200\n",
      "On data 50/200\n",
      "On data 51/200\n",
      "On data 52/200\n",
      "On data 53/200\n",
      "On data 54/200\n",
      "On data 55/200\n",
      "On data 56/200\n",
      "On data 57/200\n",
      "On data 58/200\n",
      "On data 59/200\n",
      "On data 60/200\n",
      "On data 61/200\n",
      "On data 62/200\n",
      "On data 63/200\n",
      "On data 64/200\n",
      "On data 65/200\n",
      "On data 66/200\n",
      "On data 67/200\n",
      "On data 68/200\n",
      "On data 69/200\n",
      "On data 70/200\n",
      "On data 71/200\n",
      "On data 72/200\n",
      "On data 73/200\n",
      "On data 74/200\n",
      "On data 75/200\n",
      "On data 76/200\n",
      "On data 77/200\n",
      "On data 78/200\n",
      "On data 79/200\n",
      "On data 80/200\n",
      "On data 81/200\n",
      "On data 82/200\n",
      "On data 83/200\n",
      "On data 84/200\n",
      "On data 85/200\n",
      "On data 86/200\n",
      "On data 87/200\n",
      "On data 88/200\n",
      "On data 89/200\n",
      "On data 90/200\n",
      "On data 91/200\n",
      "On data 92/200\n",
      "On data 93/200\n",
      "On data 94/200\n",
      "On data 95/200\n",
      "On data 96/200\n",
      "On data 97/200\n",
      "On data 98/200\n",
      "On data 99/200\n",
      "On data 100/200\n",
      "On data 101/200\n",
      "On data 102/200\n",
      "On data 103/200\n",
      "On data 104/200\n",
      "On data 105/200\n",
      "On data 106/200\n",
      "On data 107/200\n",
      "On data 108/200\n",
      "On data 109/200\n",
      "On data 110/200\n",
      "On data 111/200\n",
      "On data 112/200\n",
      "On data 113/200\n",
      "On data 114/200\n",
      "On data 115/200\n",
      "On data 116/200\n",
      "On data 117/200\n",
      "On data 118/200\n",
      "On data 119/200\n",
      "On data 120/200\n",
      "On data 121/200\n",
      "On data 122/200\n",
      "On data 123/200\n",
      "On data 124/200\n",
      "On data 125/200\n",
      "On data 126/200\n",
      "On data 127/200\n",
      "Valid | Loss:650.74107 MRR: 0.280 NDCG: 0.287\n",
      "saving model with MRR 0.280 NDCG: 0.287\n"
     ]
    }
   ],
   "source": [
    "print(\"start training......\")\n",
    "training_mask(model=model,\n",
    "              epoch=4, #TODO: 40,\n",
    "              train=train_loader_mask,\n",
    "              valid=valid_loader_mask,\n",
    "              quote_tensor=quote_embeddings,\n",
    "              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, quote_tensor, device):\n",
    "    print(\"start test......\")\n",
    "    model.eval()\n",
    "    t_batch = len(test_loader)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    quote_tensor = quote_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_MRR, total_NDCG, total_ranks = 0, 0, 0, 0\n",
    "        total_recall_1, total_recall_3, total_recall_5, total_recall_10, total_recall_20, total_recall_30 = 0, 0, 0, 0, 0, 0\n",
    "        total_recall_100, total_recall_300, total_recall_500, total_recall_1000 = 0, 0, 0, 0\n",
    "        all_ranks = []\n",
    "        for i, (input_ids, token_type_ids, attention_masks, mask_ids, labels) in enumerate(test_loader):\n",
    "\n",
    "            print(f\"On data {i}/{len(test_loader)}\")\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            mask_ids = mask_ids.to(device, dtype=torch.long)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "            outputs = model(input_ids, token_type_ids, attention_masks, mask_ids, quote_tensor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            ranks = rank_gold(outputs, labels)\n",
    "            all_ranks.extend(ranks)\n",
    "            MRR = np.average([1.0 / (r + 1) for r in ranks])\n",
    "            NDCG = get_NDCG(ranks)\n",
    "            recall_1, recall_3, recall_5, recall_10, recall_20, recall_30, recall_100, recall_300, recall_500, recall_1000 = recall(\n",
    "                outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_MRR += MRR\n",
    "            total_NDCG += NDCG\n",
    "            total_ranks += np.sum(ranks)\n",
    "            total_recall_1 += recall_1\n",
    "            total_recall_3 += recall_3\n",
    "            total_recall_5 += recall_5\n",
    "            total_recall_10 += recall_10\n",
    "            total_recall_20 += recall_20\n",
    "            total_recall_30 += recall_30\n",
    "            total_recall_100 += recall_100\n",
    "            total_recall_300 += recall_300\n",
    "            total_recall_500 += recall_500\n",
    "            total_recall_1000 += recall_1000\n",
    "        print(len(all_ranks))\n",
    "        print(\n",
    "            \"Test | Loss:{:.5f} MRR: {:.3f} NDCG: {:.3f} Mean Rank: {:.0f} Median Rank: {:.0f} Variance: {:.0f}\"\n",
    "            .format(total_loss, total_MRR / t_batch,\n",
    "                    total_NDCG / t_batch, np.mean(all_ranks),\n",
    "                    np.median(all_ranks)+1,\n",
    "                    np.std(all_ranks)))\n",
    "        print(\n",
    "            \"Recall@1:{:.4f} Recall@3: {:.4f} Recall@5: {:.4f} Recall@10: {:.4f} Recall@20: {:.4f} Recall@30: {:.4f} Recall@100: {:.4f} Recall@300: {:.4f} Recall@500: {:.4f} Recall@1000: {:.4f}\"\n",
    "            .format(\n",
    "                total_recall_1 / len(y_test), total_recall_3 / len(y_test),\n",
    "                total_recall_5 / len(y_test), total_recall_10 / len(y_test),\n",
    "                total_recall_20 / len(y_test), total_recall_30 / len(y_test),\n",
    "                total_recall_100 / len(y_test), total_recall_300 / len(y_test),\n",
    "                total_recall_500 / len(y_test),\n",
    "                total_recall_1000 / len(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archived video of that message here: biblical evidence for natural law listen to the scripture: \"for when the gentiles, which have not the law, do by nature the things contained in the law, these, having not the law, are \n",
      " which shew the work of the law written in their hearts, their conscience also bearing witness, and their thoughts the mean while accusing or else excusing one another.\" the great theologians and bible scholars of yesteryear all understood the\n"
     ]
    }
   ],
   "source": [
    "print(test_former[0])\n",
    "print(test_latter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading test tensor......\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"loading test tensor......\")\n",
    "test_input_ids, test_token_type_ids, test_attention_masks, test_mask_ids = make_context_tensors(test_former, test_latter)\n",
    "test_dataset_mask = Dataset_Mask(input_ids=test_input_ids,\n",
    "                                 token_type_ids=test_token_type_ids,\n",
    "                                 attention_masks=test_attention_masks,\n",
    "                                 mask_ids=test_mask_ids,\n",
    "                                 y=y_test)\n",
    "test_loader_mask = DataLoader(dataset=test_dataset_mask,\n",
    "                              batch_size=64,\n",
    "                              num_workers=0) #TODO: 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] archived video of that message here : biblical evidence for natural law listen to the scripture : \" for when the gentiles, which have not the law, do by nature the things contained in the law, these, having not the law, are [MASK] which shew the work of the law written in their hearts, their conscience also bearing witness, and their thoughts the mean while accusing or else excusing one another. \" the great theologians and bible scholars of yesteryear all understood the [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# decode what the test input is\n",
    "decoded_sentence = tokenizer.decode(test_input_ids[0])\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 6 seconds for each test --> 20 minutes run time :(\n",
    "print('loading model ...')\n",
    "model = torch.load('./model/model_english.model')\n",
    "model.to(device)\n",
    "test(model=model,\n",
    "     test_loader=test_loader_mask,\n",
    "     quote_tensor=quote_embeddings,\n",
    "     device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readable testing of what the model does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 2\n",
    "\n",
    "test_loader_mask_0 = DataLoader(dataset=test_dataset_mask,\n",
    "                              batch_size=TEST_BATCH_SIZE,\n",
    "                              num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED:\n",
    "\n",
    "# only test the 0th one\n",
    "def OLD_test_readable_0(model, test_loader, quote_tensor, device):\n",
    "    print(\"start test......\")\n",
    "    model.eval()\n",
    "    t_batch = len(test_loader)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    quote_tensor = quote_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_MRR, total_NDCG, total_ranks = 0, 0, 0, 0\n",
    "        total_recall_1, total_recall_3, total_recall_5, total_recall_10, total_recall_20, total_recall_30 = 0, 0, 0, 0, 0, 0\n",
    "        total_recall_100, total_recall_300, total_recall_500, total_recall_1000 = 0, 0, 0, 0\n",
    "        all_ranks = []\n",
    "        for i, (input_ids, token_type_ids, attention_masks, mask_ids, labels) in enumerate(test_loader):\n",
    "            if i > 0:\n",
    "                break\n",
    "\n",
    "            print(f\"On data {i}/{len(test_loader)}\")\n",
    "\n",
    "            # decode what the test input is\n",
    "            # 64 things in the batch\n",
    "            print(f\"input shape: {input_ids.shape}\")\n",
    "            # Decode the input_ids of sentence 0\n",
    "            # this is decoding the sentence tokens --> words\n",
    "            decoded_sentence = tokenizer.decode(input_ids[0], attention_mask=attention_masks[0])\n",
    "            print(\"input0:\", decoded_sentence)\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            mask_ids = mask_ids.to(device, dtype=torch.long)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "            outputs = model(input_ids, token_type_ids, attention_masks, mask_ids, quote_tensor)\n",
    "            \n",
    "            print(f\"outputs.shape: {outputs.shape}\")\n",
    "            # Print the list of NL sentences arranged by rank for input_ids[0]\n",
    "            print(f\"labels: {labels}\")\n",
    "            print(f\"outputs for input0: {outputs}\")\n",
    "            print(\"Predicted Sentences for input0:\")\n",
    "            \n",
    "            # TODO: figure this out\n",
    "            ps = outputs.data.cpu().numpy()\n",
    "            print(f\"len(ps): {len(ps)}\")\n",
    "            gs = labels.cpu().numpy()\n",
    "            i=0 # for i in range(len(ps)):\n",
    "            predict = ps[i] # predictions corresponding to input 0??\n",
    "            gold_index = gs[i] # \n",
    "            print(f\"predict: {predict}\\ngold: {gold_index}\")\n",
    "            predict_value = predict[gold_index]\n",
    "            predict_sort = sorted(predict, reverse=True)\n",
    "            predict_index = predict_sort.index(predict_value)\n",
    "            if predict_index == -1:\n",
    "                break\n",
    "\n",
    "        #     loss = criterion(outputs, labels)\n",
    "        #     ranks = rank_gold(outputs, labels)\n",
    "\n",
    "        #     print(f\"ranks = {ranks}\")\n",
    "\n",
    "        #     all_ranks.extend(ranks)\n",
    "        #     MRR = np.average([1.0 / (r + 1) for r in ranks])\n",
    "        #     NDCG = get_NDCG(ranks)\n",
    "        #     recall_1, recall_3, recall_5, recall_10, recall_20, recall_30, recall_100, recall_300, recall_500, recall_1000 = recall(\n",
    "        #         outputs, labels)\n",
    "        #     total_loss += loss.item()\n",
    "        #     total_MRR += MRR\n",
    "        #     total_NDCG += NDCG\n",
    "        #     total_ranks += np.sum(ranks)\n",
    "        #     total_recall_1 += recall_1\n",
    "        #     total_recall_3 += recall_3\n",
    "        #     total_recall_5 += recall_5\n",
    "        #     total_recall_10 += recall_10\n",
    "        #     total_recall_20 += recall_20\n",
    "        #     total_recall_30 += recall_30\n",
    "        #     total_recall_100 += recall_100\n",
    "        #     total_recall_300 += recall_300\n",
    "        #     total_recall_500 += recall_500\n",
    "        #     total_recall_1000 += recall_1000\n",
    "        # print(f\"len(all_ranks) = {len(all_ranks)}\")\n",
    "        # print(\n",
    "        #     \"Test | Loss:{:.5f} MRR: {:.3f} NDCG: {:.3f} Mean Rank: {:.0f} Median Rank: {:.0f} Variance: {:.0f}\"\n",
    "        #     .format(total_loss, total_MRR / t_batch,\n",
    "        #             total_NDCG / t_batch, np.mean(all_ranks),\n",
    "        #             np.median(all_ranks)+1,\n",
    "        #             np.std(all_ranks)))\n",
    "        # print(\n",
    "        #     \"Recall@1:{:.4f} Recall@3: {:.4f} Recall@5: {:.4f} Recall@10: {:.4f} Recall@20: {:.4f} Recall@30: {:.4f} Recall@100: {:.4f} Recall@300: {:.4f} Recall@500: {:.4f} Recall@1000: {:.4f}\"\n",
    "        #     .format(\n",
    "        #         total_recall_1 / len(y_test), total_recall_3 / len(y_test),\n",
    "        #         total_recall_5 / len(y_test), total_recall_10 / len(y_test),\n",
    "        #         total_recall_20 / len(y_test), total_recall_30 / len(y_test),\n",
    "        #         total_recall_100 / len(y_test), total_recall_300 / len(y_test),\n",
    "        #         total_recall_500 / len(y_test),\n",
    "        #         total_recall_1000 / len(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start test......\n",
      "On data 0/6386\n",
      "input shape: torch.Size([2, 150])\n",
      "input0: [CLS] archived video of that message here : biblical evidence for natural law listen to the scripture : \" for when the gentiles, which have not the law, do by nature the things contained in the law, these, having not the law, are [MASK] which shew the work of the law written in their hearts, their conscience also bearing witness, and their thoughts the mean while accusing or else excusing one another. \" the great theologians and bible scholars of yesteryear all understood the [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "outputs.shape: torch.Size([2, 6108])\n",
      "labels: tensor([ 76, 867])\n",
      "outputs for input0: tensor([[38.2892, 38.7050, 37.6607,  ..., 39.3191, 31.3672, 39.2444],\n",
      "        [41.1382, 42.0630, 48.7875,  ..., 44.9262, 45.1693, 47.5984]])\n",
      "Predicted Sentences for input0:\n",
      "len(ps): 2\n",
      "predict: [38.28918  38.704952 37.66069  ... 39.319077 31.367249 39.244446]\n",
      "gold: 76\n"
     ]
    }
   ],
   "source": [
    "test_readable_0(model=model,\n",
    "     test_loader=test_loader_mask_0,\n",
    "     quote_tensor=quote_embeddings,\n",
    "     device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model\n",
    "test_loader = test_loader_mask\n",
    "quote_tensor = quote_embeddings\n",
    "device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12771"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start test......\n",
      "On data 0/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 1/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 2/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 3/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 4/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 5/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 6/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 7/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 8/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 9/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 10/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 11/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 12/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 13/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 14/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 15/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 16/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 17/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 18/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 19/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 20/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 21/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 22/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 23/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 24/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 25/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 26/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 27/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 28/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 29/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 30/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 31/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 32/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 33/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 34/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 35/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 36/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 37/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 38/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 39/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 40/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 41/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 42/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 43/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 44/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 45/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 46/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 47/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 48/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 49/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 50/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 51/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 52/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 53/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 54/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 55/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 56/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 57/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 58/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 59/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 60/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 61/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 62/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 63/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 64/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 65/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 66/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 67/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 68/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 69/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 70/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 71/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 72/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 73/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 74/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 75/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 76/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 77/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 78/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 79/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 80/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 81/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 82/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 83/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 84/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 85/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 86/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 87/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 88/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 89/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 90/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 91/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 92/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 93/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 94/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 95/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 96/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 97/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 98/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 99/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 100/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 101/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 102/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 103/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 104/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 105/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 106/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 107/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 108/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 109/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 110/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 111/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 112/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 113/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 114/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 115/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 116/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 117/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 118/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 119/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 120/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 121/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 122/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 123/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 124/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 125/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 126/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 127/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 128/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 129/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 130/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 131/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 132/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 133/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 134/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 135/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 136/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 137/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 138/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 139/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 140/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 141/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 142/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 143/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 144/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 145/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 146/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 147/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 148/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 149/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 150/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 151/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 152/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 153/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 154/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 155/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 156/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 157/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 158/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 159/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 160/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 161/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 162/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 163/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 164/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 165/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 166/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 167/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 168/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 169/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 170/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 171/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 172/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 173/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 174/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 175/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 176/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 177/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 178/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 179/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 180/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 181/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 182/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 183/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 184/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 185/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 186/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 187/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 188/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 189/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 190/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 191/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 192/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 193/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 194/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 195/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 196/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 197/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 198/200\n",
      "input shape: torch.Size([64, 150])\n",
      "On data 199/200\n",
      "input shape: torch.Size([35, 150])\n"
     ]
    }
   ],
   "source": [
    "print(\"start test......\")\n",
    "model.eval()\n",
    "t_batch = len(test_loader)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "quote_tensor = quote_tensor.to(device)\n",
    "\n",
    "\n",
    "all_dfs = []\n",
    "with torch.no_grad():\n",
    "    # all_quote_ranks = []\n",
    "    # all_quote_gs = []\n",
    "    for i, (input_ids, token_type_ids, attention_masks, mask_ids, labels) in enumerate(test_loader):\n",
    "        print(f\"On data {i}/{len(test_loader)}\")\n",
    "\n",
    "        # decode what the test input is\n",
    "        # 64 things in the batch\n",
    "        print(f\"input shape: {input_ids.shape}\")\n",
    "        # Decode the input_ids of sentence 0\n",
    "        # this is decoding the sentence tokens --> words\n",
    "        decoded_sentence = tokenizer.decode(input_ids[0], attention_mask=attention_masks[0])\n",
    "        # print(\"input0:\", decoded_sentence)\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        mask_ids = mask_ids.to(device, dtype=torch.long)\n",
    "        labels = labels.to(device, dtype=torch.long)\n",
    "        outputs = model(input_ids, token_type_ids, attention_masks, mask_ids, quote_tensor)\n",
    "        \n",
    "        # print(f\"outputs.shape: {outputs.shape}\")\n",
    "        # Print the list of NL sentences arranged by rank for input_ids[0]\n",
    "        # print(f\"labels: {labels}\")\n",
    "        # print(f\"outputs for input0: {outputs}\")\n",
    "        # print(\"Predicted Sentences for input0:\")\n",
    "        \n",
    "        # TODO: figure this out\n",
    "        ps = outputs.data.cpu().numpy()\n",
    "        # print(f\"ps = {ps}\")\n",
    "        # print(f\"len(ps): {len(ps)}\")\n",
    "        # in descending order\n",
    "        sorted_indices = np.argsort(np.array(ps)*-1)\n",
    "        # print(f\"Sorted Indices: {sorted_indices}\")\n",
    "        # gs (gold standard?) represents the actual data number? of the quote in question\n",
    "        gs = labels.cpu().numpy()\n",
    "        # print(f\"gs = {gs}\")\n",
    "\n",
    "        # cool, now I want the actual corresponding sentence.\n",
    "        start = i*TEST_BATCH_SIZE\n",
    "        end = min(len(y_test), start + len(labels))\n",
    "\n",
    "        # create df from this part of data\n",
    "        batch_df_map = {}\n",
    "\n",
    "        batch_df_map[\"test_former\"] = test_former[start:end]\n",
    "        batch_df_map[\"test_latter\"] = test_latter[start:end]\n",
    "        batch_df_map[\"gs\"] = gs\n",
    "        # top k actual quotes\n",
    "        top_k = 20\n",
    "        for k in range(top_k):\n",
    "            # kth most relevant result\n",
    "            batch_df_map[f\"res_{k+1}\"] = [all_quotes[k_idx] for k_idx in sorted_indices[:,k]]\n",
    "        \n",
    "        # predicted ranks - list of lists\n",
    "        batch_df_map[\"sorted_indices\"] = sorted_indices.tolist()\n",
    "\n",
    "        batch_df = pd.DataFrame(batch_df_map)\n",
    "\n",
    "        # all_quote_ranks.append(sorted_indices, gs)\n",
    "        all_dfs.append(batch_df)\n",
    "\n",
    "final_df = pd.concat(all_dfs # , ignore_index=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_former</th>\n",
       "      <th>test_latter</th>\n",
       "      <th>gs</th>\n",
       "      <th>res_1</th>\n",
       "      <th>res_2</th>\n",
       "      <th>res_3</th>\n",
       "      <th>res_4</th>\n",
       "      <th>res_5</th>\n",
       "      <th>res_6</th>\n",
       "      <th>res_7</th>\n",
       "      <th>...</th>\n",
       "      <th>res_12</th>\n",
       "      <th>res_13</th>\n",
       "      <th>res_14</th>\n",
       "      <th>res_15</th>\n",
       "      <th>res_16</th>\n",
       "      <th>res_17</th>\n",
       "      <th>res_18</th>\n",
       "      <th>res_19</th>\n",
       "      <th>res_20</th>\n",
       "      <th>sorted_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>archived video of that message here: biblical ...</td>\n",
       "      <td>which shew the work of the law written in the...</td>\n",
       "      <td>76</td>\n",
       "      <td>a law unto themselves.</td>\n",
       "      <td>a stiff-necked people.</td>\n",
       "      <td>a government of laws, and not of men.</td>\n",
       "      <td>to keep law and order.</td>\n",
       "      <td>the world, the flesh, and the devil.</td>\n",
       "      <td>from the strife of tongues.</td>\n",
       "      <td>tossing their heads in sprightly dance.</td>\n",
       "      <td>...</td>\n",
       "      <td>tattlers also and busybodies, speaking things ...</td>\n",
       "      <td>let well alone.</td>\n",
       "      <td>the cattle upon a thousand hills.</td>\n",
       "      <td>treated like dirt.</td>\n",
       "      <td>by evil report and good report.</td>\n",
       "      <td>the defenders of democracy.</td>\n",
       "      <td>the palpable obscure.</td>\n",
       "      <td>the right divine of kings to govern wrong.</td>\n",
       "      <td>words of truth and soberness.</td>\n",
       "      <td>[76, 170, 60, 5314, 4892, 1625, 5359, 5610, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in the memories of his people, in the rest of ...</td>\n",
       "      <td>\"blessed are the merciful! for they shall obt...</td>\n",
       "      <td>867</td>\n",
       "      <td>blessed are the peacemakers, for they shall be...</td>\n",
       "      <td>blessed are they that mourn for they shall be ...</td>\n",
       "      <td>blessed are the merciful: for they shall obtai...</td>\n",
       "      <td>blessed are the merciful, for they will be sho...</td>\n",
       "      <td>blessed are they which do hunger and thirst af...</td>\n",
       "      <td>blessed are the meek: for they shall inherit t...</td>\n",
       "      <td>blessed are the meek, for they will inherit th...</td>\n",
       "      <td>...</td>\n",
       "      <td>the sons of god saw the daughters of men, that...</td>\n",
       "      <td>blessed are ye that hunger now: for ye shall b...</td>\n",
       "      <td>blessed are the young, for they shall inherit ...</td>\n",
       "      <td>and the angel said unto them, fear not: for, b...</td>\n",
       "      <td>blessed are the undefiled in the way, who walk...</td>\n",
       "      <td>but jesus turning unto them said, daughters of...</td>\n",
       "      <td>and god blessed noah and his sons, and said un...</td>\n",
       "      <td>and the lord commended the unjust steward, bec...</td>\n",
       "      <td>blessings brighten as they take their flight.</td>\n",
       "      <td>[870, 875, 869, 868, 876, 867, 866, 5109, 1964...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the human species depends on maximizing the po...</td>\n",
       "      <td>but we do no even have to be certain of this....</td>\n",
       "      <td>2784</td>\n",
       "      <td>injustice anywhere is a threat to justice ever...</td>\n",
       "      <td>there may be times when we are powerless to pr...</td>\n",
       "      <td>the arc of the moral universe is long, but it ...</td>\n",
       "      <td>true peace is not merely the absence of tensio...</td>\n",
       "      <td>two possibilities exist: either we are alone i...</td>\n",
       "      <td>our lives begin to end the day we become silen...</td>\n",
       "      <td>rightful liberty is unobstructed action accord...</td>\n",
       "      <td>...</td>\n",
       "      <td>never in the field of human conflict was so mu...</td>\n",
       "      <td>is designed to make lies sound truthful and mu...</td>\n",
       "      <td>revenge is a kind of wild justice.</td>\n",
       "      <td>we cannot solve our problems with the same thi...</td>\n",
       "      <td>all we are saying is give peace a chance.</td>\n",
       "      <td>it is better to be violent, if there is violen...</td>\n",
       "      <td>it is our duty to fight for our freedom.</td>\n",
       "      <td>in our age there is no such thing as 'keeping ...</td>\n",
       "      <td>there is no fate but what we make.</td>\n",
       "      <td>[2784, 5047, 4337, 5379, 5413, 3788, 3991, 255...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>said that he was 'carnal, sold under sin' . no...</td>\n",
       "      <td>this is my experience. i do many things i oug...</td>\n",
       "      <td>1545</td>\n",
       "      <td>vengeance is mine, and i will repay.</td>\n",
       "      <td>vengeance is mine, i will repay, saith the lord.</td>\n",
       "      <td>vengeance is mine; i will repay.</td>\n",
       "      <td>i have sinned in that i have betrayed the inno...</td>\n",
       "      <td>for i acknowledge my transgressions: and my si...</td>\n",
       "      <td>i find then a law, that, when i would do good,...</td>\n",
       "      <td>my punishment is greater than i can bear.</td>\n",
       "      <td>...</td>\n",
       "      <td>you cannot serve god and mammon.</td>\n",
       "      <td>he that hateth me hateth my father also.</td>\n",
       "      <td>he that loveth father or mother more than me i...</td>\n",
       "      <td>ye have not yet resisted unto blood, striving ...</td>\n",
       "      <td>i make peace and create evil.</td>\n",
       "      <td>i have not come to call the righteous, but sin...</td>\n",
       "      <td>i can do all things through christ which stren...</td>\n",
       "      <td>thou shalt not commit adultery.</td>\n",
       "      <td>avenge not yourselves, but rather give place u...</td>\n",
       "      <td>[5442, 5443, 5444, 2296, 1510, 2241, 3425, 151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a female student run up to a man wearing a sik...</td>\n",
       "      <td>over the exhortations of the speaker, the vid...</td>\n",
       "      <td>320</td>\n",
       "      <td>leave my mother alone!</td>\n",
       "      <td>give me your tired, your poor, your huddled ma...</td>\n",
       "      <td>take my wife - please!</td>\n",
       "      <td>give me liberty, or give me death.</td>\n",
       "      <td>god save us all.</td>\n",
       "      <td>thank god, i have done my duty.</td>\n",
       "      <td>god, who am i?</td>\n",
       "      <td>...</td>\n",
       "      <td>you have to be with you.</td>\n",
       "      <td>three's a crowd.</td>\n",
       "      <td>save us from our friends.</td>\n",
       "      <td>it is our duty to fight for our freedom.</td>\n",
       "      <td>who dares, win.</td>\n",
       "      <td>so what? they're muslim.</td>\n",
       "      <td>i'm going to do what i want to do.</td>\n",
       "      <td>i hope we answer the alarm clock and take this...</td>\n",
       "      <td>mr. gorbachev, tear down this wall!</td>\n",
       "      <td>[3065, 1666, 4246, 1659, 1719, 4278, 1726, 643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>the second world war and is often suggested to...</td>\n",
       "      <td>it has been anticipated and planned for by mi...</td>\n",
       "      <td>3538</td>\n",
       "      <td>no religious test shall ever be required as a ...</td>\n",
       "      <td>the government of the united states is not, in...</td>\n",
       "      <td>let us with caution indulge the supposition th...</td>\n",
       "      <td>therefore we conclude that a man is justified ...</td>\n",
       "      <td>religion is regarded by the common people as t...</td>\n",
       "      <td>reason and experience both forbid us to expect...</td>\n",
       "      <td>faith is the substance of things hoped for, th...</td>\n",
       "      <td>...</td>\n",
       "      <td>the sabbath was made for man, and not man for ...</td>\n",
       "      <td>with or without religion, good people can beha...</td>\n",
       "      <td>be deprived of life, liberty, or property with...</td>\n",
       "      <td>everything not forbidden is compulsory.</td>\n",
       "      <td>respect every religion.</td>\n",
       "      <td>there is no conflict between science and relig...</td>\n",
       "      <td>if any man will do his will, he shall know of ...</td>\n",
       "      <td>for sin shall not have dominion over you: for ...</td>\n",
       "      <td>scientology … is not a religion.</td>\n",
       "      <td>[3538, 4482, 3117, 5093, 3959, 3947, 1406, 759...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>her new book, ann coulter says there is only o...</td>\n",
       "      <td>coulter wrote in her new tome, \"in trump we t...</td>\n",
       "      <td>3802</td>\n",
       "      <td>treated like dirt.</td>\n",
       "      <td>the punishment he deserves.</td>\n",
       "      <td>to give the devil his due.</td>\n",
       "      <td>nothing for nothing.</td>\n",
       "      <td>a feast for the eyes.</td>\n",
       "      <td>as ill-luck would have it.</td>\n",
       "      <td>let well alone.</td>\n",
       "      <td>...</td>\n",
       "      <td>killing no murder.</td>\n",
       "      <td>from the strife of tongues.</td>\n",
       "      <td>in spite of my teeth.</td>\n",
       "      <td>as good luck would have it.</td>\n",
       "      <td>tossing their heads in sprightly dance.</td>\n",
       "      <td>vengeance is mine; i will repay.</td>\n",
       "      <td>a feast of fat things.</td>\n",
       "      <td>out of the frying pan into the fire.</td>\n",
       "      <td>resolved to repent.</td>\n",
       "      <td>[5372, 4716, 5309, 3592, 32, 680, 3119, 170, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>thugs, and anti-heroes who shadow forth every ...</td>\n",
       "      <td>their preferred depictions of supposedly huma...</td>\n",
       "      <td>5674</td>\n",
       "      <td>who wants to live forever?</td>\n",
       "      <td>turn on, tune in, drop out.</td>\n",
       "      <td>tears in heaven.</td>\n",
       "      <td>happy girls are the prettiest.</td>\n",
       "      <td>life is magic.</td>\n",
       "      <td>sex and drugs and rock and roll.</td>\n",
       "      <td>three's a crowd.</td>\n",
       "      <td>...</td>\n",
       "      <td>dreams do come true sometimes.</td>\n",
       "      <td>i'm the luckiest girl in the world.</td>\n",
       "      <td>love is a story.</td>\n",
       "      <td>dream on, dream on.</td>\n",
       "      <td>let love rule.</td>\n",
       "      <td>live a party.</td>\n",
       "      <td>the times they are a-changin.</td>\n",
       "      <td>love is all you need.</td>\n",
       "      <td>it's kind of fun to do the impossible.</td>\n",
       "      <td>[5817, 5407, 4264, 1804, 3149, 4046, 5245, 545...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>mabel could not trust herself to say anything;...</td>\n",
       "      <td>of what untold comfort are you to the mourner...</td>\n",
       "      <td>1872</td>\n",
       "      <td>as ill-luck would have it.</td>\n",
       "      <td>let well alone.</td>\n",
       "      <td>as good luck would have it.</td>\n",
       "      <td>to give the devil his due.</td>\n",
       "      <td>by any stretch of the imagination.</td>\n",
       "      <td>time will tell.</td>\n",
       "      <td>victory or westminster abbey.</td>\n",
       "      <td>...</td>\n",
       "      <td>to tell tales out of school.</td>\n",
       "      <td>a hit, a very palpable hit.</td>\n",
       "      <td>necessity knows no law.</td>\n",
       "      <td>every man to his trade.</td>\n",
       "      <td>hypotheses non fingo.</td>\n",
       "      <td>from the strife of tongues.</td>\n",
       "      <td>a snapper-up of unconsidered trifles.</td>\n",
       "      <td>a fool in love.</td>\n",
       "      <td>in spite of my teeth.</td>\n",
       "      <td>[680, 3119, 673, 5309, 980, 5273, 5456, 4564, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>iii., sc. 1. =dominion.= here we may reign sec...</td>\n",
       "      <td>577 shaks.: macbeth, act iv., sc. 1. =doubt.=...</td>\n",
       "      <td>3254</td>\n",
       "      <td>leave no stone unturned.</td>\n",
       "      <td>love thy neighbor as thyself.</td>\n",
       "      <td>spare the rod and spoil the child.</td>\n",
       "      <td>bear and forbear.</td>\n",
       "      <td>forgive and forget.</td>\n",
       "      <td>confess and be hanged.</td>\n",
       "      <td>receive with joy.</td>\n",
       "      <td>...</td>\n",
       "      <td>cast down your bucket where you are.</td>\n",
       "      <td>go to work on an egg.</td>\n",
       "      <td>go further and fare worse.</td>\n",
       "      <td>turn over a new leaf.</td>\n",
       "      <td>strike the shepherd and the sheep will scatter.</td>\n",
       "      <td>be steadfast in the faith.</td>\n",
       "      <td>stiffen the sinews, summon up the blood.</td>\n",
       "      <td>touch not; taste not; handle not.</td>\n",
       "      <td>bear, like the turk, no brother near the throne.</td>\n",
       "      <td>[3066, 3254, 4163, 781, 1589, 1078, 3952, 1950...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12771 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          test_former  \\\n",
       "0   archived video of that message here: biblical ...   \n",
       "1   in the memories of his people, in the rest of ...   \n",
       "2   the human species depends on maximizing the po...   \n",
       "3   said that he was 'carnal, sold under sin' . no...   \n",
       "4   a female student run up to a man wearing a sik...   \n",
       "..                                                ...   \n",
       "30  the second world war and is often suggested to...   \n",
       "31  her new book, ann coulter says there is only o...   \n",
       "32  thugs, and anti-heroes who shadow forth every ...   \n",
       "33  mabel could not trust herself to say anything;...   \n",
       "34  iii., sc. 1. =dominion.= here we may reign sec...   \n",
       "\n",
       "                                          test_latter    gs  \\\n",
       "0    which shew the work of the law written in the...    76   \n",
       "1    \"blessed are the merciful! for they shall obt...   867   \n",
       "2    but we do no even have to be certain of this....  2784   \n",
       "3    this is my experience. i do many things i oug...  1545   \n",
       "4    over the exhortations of the speaker, the vid...   320   \n",
       "..                                                ...   ...   \n",
       "30   it has been anticipated and planned for by mi...  3538   \n",
       "31   coulter wrote in her new tome, \"in trump we t...  3802   \n",
       "32   their preferred depictions of supposedly huma...  5674   \n",
       "33   of what untold comfort are you to the mourner...  1872   \n",
       "34   577 shaks.: macbeth, act iv., sc. 1. =doubt.=...  3254   \n",
       "\n",
       "                                                res_1  \\\n",
       "0                              a law unto themselves.   \n",
       "1   blessed are the peacemakers, for they shall be...   \n",
       "2   injustice anywhere is a threat to justice ever...   \n",
       "3                vengeance is mine, and i will repay.   \n",
       "4                              leave my mother alone!   \n",
       "..                                                ...   \n",
       "30  no religious test shall ever be required as a ...   \n",
       "31                                 treated like dirt.   \n",
       "32                         who wants to live forever?   \n",
       "33                         as ill-luck would have it.   \n",
       "34                           leave no stone unturned.   \n",
       "\n",
       "                                                res_2  \\\n",
       "0                              a stiff-necked people.   \n",
       "1   blessed are they that mourn for they shall be ...   \n",
       "2   there may be times when we are powerless to pr...   \n",
       "3    vengeance is mine, i will repay, saith the lord.   \n",
       "4   give me your tired, your poor, your huddled ma...   \n",
       "..                                                ...   \n",
       "30  the government of the united states is not, in...   \n",
       "31                        the punishment he deserves.   \n",
       "32                        turn on, tune in, drop out.   \n",
       "33                                    let well alone.   \n",
       "34                      love thy neighbor as thyself.   \n",
       "\n",
       "                                                res_3  \\\n",
       "0               a government of laws, and not of men.   \n",
       "1   blessed are the merciful: for they shall obtai...   \n",
       "2   the arc of the moral universe is long, but it ...   \n",
       "3                    vengeance is mine; i will repay.   \n",
       "4                              take my wife - please!   \n",
       "..                                                ...   \n",
       "30  let us with caution indulge the supposition th...   \n",
       "31                         to give the devil his due.   \n",
       "32                                   tears in heaven.   \n",
       "33                        as good luck would have it.   \n",
       "34                 spare the rod and spoil the child.   \n",
       "\n",
       "                                                res_4  \\\n",
       "0                              to keep law and order.   \n",
       "1   blessed are the merciful, for they will be sho...   \n",
       "2   true peace is not merely the absence of tensio...   \n",
       "3   i have sinned in that i have betrayed the inno...   \n",
       "4                  give me liberty, or give me death.   \n",
       "..                                                ...   \n",
       "30  therefore we conclude that a man is justified ...   \n",
       "31                               nothing for nothing.   \n",
       "32                     happy girls are the prettiest.   \n",
       "33                         to give the devil his due.   \n",
       "34                                  bear and forbear.   \n",
       "\n",
       "                                                res_5  \\\n",
       "0                the world, the flesh, and the devil.   \n",
       "1   blessed are they which do hunger and thirst af...   \n",
       "2   two possibilities exist: either we are alone i...   \n",
       "3   for i acknowledge my transgressions: and my si...   \n",
       "4                                    god save us all.   \n",
       "..                                                ...   \n",
       "30  religion is regarded by the common people as t...   \n",
       "31                              a feast for the eyes.   \n",
       "32                                     life is magic.   \n",
       "33                 by any stretch of the imagination.   \n",
       "34                                forgive and forget.   \n",
       "\n",
       "                                                res_6  \\\n",
       "0                         from the strife of tongues.   \n",
       "1   blessed are the meek: for they shall inherit t...   \n",
       "2   our lives begin to end the day we become silen...   \n",
       "3   i find then a law, that, when i would do good,...   \n",
       "4                     thank god, i have done my duty.   \n",
       "..                                                ...   \n",
       "30  reason and experience both forbid us to expect...   \n",
       "31                         as ill-luck would have it.   \n",
       "32                   sex and drugs and rock and roll.   \n",
       "33                                    time will tell.   \n",
       "34                             confess and be hanged.   \n",
       "\n",
       "                                                res_7  ...  \\\n",
       "0             tossing their heads in sprightly dance.  ...   \n",
       "1   blessed are the meek, for they will inherit th...  ...   \n",
       "2   rightful liberty is unobstructed action accord...  ...   \n",
       "3           my punishment is greater than i can bear.  ...   \n",
       "4                                      god, who am i?  ...   \n",
       "..                                                ...  ...   \n",
       "30  faith is the substance of things hoped for, th...  ...   \n",
       "31                                    let well alone.  ...   \n",
       "32                                   three's a crowd.  ...   \n",
       "33                      victory or westminster abbey.  ...   \n",
       "34                                  receive with joy.  ...   \n",
       "\n",
       "                                               res_12  \\\n",
       "0   tattlers also and busybodies, speaking things ...   \n",
       "1   the sons of god saw the daughters of men, that...   \n",
       "2   never in the field of human conflict was so mu...   \n",
       "3                    you cannot serve god and mammon.   \n",
       "4                            you have to be with you.   \n",
       "..                                                ...   \n",
       "30  the sabbath was made for man, and not man for ...   \n",
       "31                                 killing no murder.   \n",
       "32                     dreams do come true sometimes.   \n",
       "33                       to tell tales out of school.   \n",
       "34               cast down your bucket where you are.   \n",
       "\n",
       "                                               res_13  \\\n",
       "0                                     let well alone.   \n",
       "1   blessed are ye that hunger now: for ye shall b...   \n",
       "2   is designed to make lies sound truthful and mu...   \n",
       "3            he that hateth me hateth my father also.   \n",
       "4                                    three's a crowd.   \n",
       "..                                                ...   \n",
       "30  with or without religion, good people can beha...   \n",
       "31                        from the strife of tongues.   \n",
       "32                i'm the luckiest girl in the world.   \n",
       "33                        a hit, a very palpable hit.   \n",
       "34                              go to work on an egg.   \n",
       "\n",
       "                                               res_14  \\\n",
       "0                   the cattle upon a thousand hills.   \n",
       "1   blessed are the young, for they shall inherit ...   \n",
       "2                  revenge is a kind of wild justice.   \n",
       "3   he that loveth father or mother more than me i...   \n",
       "4                           save us from our friends.   \n",
       "..                                                ...   \n",
       "30  be deprived of life, liberty, or property with...   \n",
       "31                              in spite of my teeth.   \n",
       "32                                   love is a story.   \n",
       "33                            necessity knows no law.   \n",
       "34                         go further and fare worse.   \n",
       "\n",
       "                                               res_15  \\\n",
       "0                                  treated like dirt.   \n",
       "1   and the angel said unto them, fear not: for, b...   \n",
       "2   we cannot solve our problems with the same thi...   \n",
       "3   ye have not yet resisted unto blood, striving ...   \n",
       "4            it is our duty to fight for our freedom.   \n",
       "..                                                ...   \n",
       "30            everything not forbidden is compulsory.   \n",
       "31                        as good luck would have it.   \n",
       "32                                dream on, dream on.   \n",
       "33                            every man to his trade.   \n",
       "34                              turn over a new leaf.   \n",
       "\n",
       "                                               res_16  \\\n",
       "0                     by evil report and good report.   \n",
       "1   blessed are the undefiled in the way, who walk...   \n",
       "2           all we are saying is give peace a chance.   \n",
       "3                       i make peace and create evil.   \n",
       "4                                     who dares, win.   \n",
       "..                                                ...   \n",
       "30                            respect every religion.   \n",
       "31            tossing their heads in sprightly dance.   \n",
       "32                                     let love rule.   \n",
       "33                              hypotheses non fingo.   \n",
       "34    strike the shepherd and the sheep will scatter.   \n",
       "\n",
       "                                               res_17  \\\n",
       "0                         the defenders of democracy.   \n",
       "1   but jesus turning unto them said, daughters of...   \n",
       "2   it is better to be violent, if there is violen...   \n",
       "3   i have not come to call the righteous, but sin...   \n",
       "4                            so what? they're muslim.   \n",
       "..                                                ...   \n",
       "30  there is no conflict between science and relig...   \n",
       "31                   vengeance is mine; i will repay.   \n",
       "32                                      live a party.   \n",
       "33                        from the strife of tongues.   \n",
       "34                         be steadfast in the faith.   \n",
       "\n",
       "                                               res_18  \\\n",
       "0                               the palpable obscure.   \n",
       "1   and god blessed noah and his sons, and said un...   \n",
       "2            it is our duty to fight for our freedom.   \n",
       "3   i can do all things through christ which stren...   \n",
       "4                  i'm going to do what i want to do.   \n",
       "..                                                ...   \n",
       "30  if any man will do his will, he shall know of ...   \n",
       "31                             a feast of fat things.   \n",
       "32                      the times they are a-changin.   \n",
       "33              a snapper-up of unconsidered trifles.   \n",
       "34           stiffen the sinews, summon up the blood.   \n",
       "\n",
       "                                               res_19  \\\n",
       "0          the right divine of kings to govern wrong.   \n",
       "1   and the lord commended the unjust steward, bec...   \n",
       "2   in our age there is no such thing as 'keeping ...   \n",
       "3                     thou shalt not commit adultery.   \n",
       "4   i hope we answer the alarm clock and take this...   \n",
       "..                                                ...   \n",
       "30  for sin shall not have dominion over you: for ...   \n",
       "31               out of the frying pan into the fire.   \n",
       "32                              love is all you need.   \n",
       "33                                    a fool in love.   \n",
       "34                  touch not; taste not; handle not.   \n",
       "\n",
       "                                               res_20  \\\n",
       "0                       words of truth and soberness.   \n",
       "1       blessings brighten as they take their flight.   \n",
       "2                  there is no fate but what we make.   \n",
       "3   avenge not yourselves, but rather give place u...   \n",
       "4                 mr. gorbachev, tear down this wall!   \n",
       "..                                                ...   \n",
       "30                   scientology … is not a religion.   \n",
       "31                                resolved to repent.   \n",
       "32             it's kind of fun to do the impossible.   \n",
       "33                              in spite of my teeth.   \n",
       "34   bear, like the turk, no brother near the throne.   \n",
       "\n",
       "                                       sorted_indices  \n",
       "0   [76, 170, 60, 5314, 4892, 1625, 5359, 5610, 10...  \n",
       "1   [870, 875, 869, 868, 876, 867, 866, 5109, 1964...  \n",
       "2   [2784, 5047, 4337, 5379, 5413, 3788, 3991, 255...  \n",
       "3   [5442, 5443, 5444, 2296, 1510, 2241, 3425, 151...  \n",
       "4   [3065, 1666, 4246, 1659, 1719, 4278, 1726, 643...  \n",
       "..                                                ...  \n",
       "30  [3538, 4482, 3117, 5093, 3959, 3947, 1406, 759...  \n",
       "31  [5372, 4716, 5309, 3592, 32, 680, 3119, 170, 1...  \n",
       "32  [5817, 5407, 4264, 1804, 3149, 4046, 5245, 545...  \n",
       "33  [680, 3119, 673, 5309, 980, 5273, 5456, 4564, ...  \n",
       "34  [3066, 3254, 4163, 781, 1589, 1078, 3952, 1950...  \n",
       "\n",
       "[12771 rows x 24 columns]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_small = final_df[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AllQuotes</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'nothing tastes as good as skinny feels.'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'please, sir, i want some more.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a babe in a house is a well-spring of pleasure.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a beggarly account of empty boxes.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a blonde to make a bishop kick a hole in a sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>your task is not to seek for love, but merely ...</td>\n",
       "      <td>6103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>your time is limited, so don't waste it living...</td>\n",
       "      <td>6104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>your word is your bond.</td>\n",
       "      <td>6105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>your work is going to fill a large part of you...</td>\n",
       "      <td>6106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6107</th>\n",
       "      <td>youth is a blunder; manhood a struggle; old ag...</td>\n",
       "      <td>6107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              AllQuotes    gs\n",
       "0             'nothing tastes as good as skinny feels.'     0\n",
       "1                      'please, sir, i want some more.'     1\n",
       "2       a babe in a house is a well-spring of pleasure.     2\n",
       "3                    a beggarly account of empty boxes.     3\n",
       "4     a blonde to make a bishop kick a hole in a sta...     4\n",
       "...                                                 ...   ...\n",
       "6103  your task is not to seek for love, but merely ...  6103\n",
       "6104  your time is limited, so don't waste it living...  6104\n",
       "6105                            your word is your bond.  6105\n",
       "6106  your work is going to fill a large part of you...  6106\n",
       "6107  youth is a blunder; manhood a struggle; old ag...  6107\n",
       "\n",
       "[6108 rows x 2 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the training data too\n",
    "quotes_mapping = pd.DataFrame({\"AllQuotes\":all_quotes, \"gs\":np.arange(len(all_quotes))})\n",
    "quotes_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out to excel\n",
    "# with pd.ExcelWriter(\"QuoteR_output.xlsx\") as writer:\n",
    "#     # write each df to separate sheet\n",
    "#     final_df.to_excel(writer, sheet_name=\"all_results\", index=False),\n",
    "#     test_data.to_excel(writer, sheet_name=\"test_data\", index=False),\n",
    "#     quotes_mapping.to_excel(writer, sheet_name=\"quotes_mapping\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out to parquet (more space efficient)\n",
    "quote_retrievals_fname= \"quote_retrievals\"\n",
    "quotes_mapping_fname = \"quotes_mapping\"\n",
    "out_dirname = \"out\"\n",
    "\n",
    "final_df.to_parquet(f\"{out_dirname}/{quote_retrievals_fname}.parquet\", index=False)\n",
    "quotes_mapping.to_parquet(f\"{out_dirname}/{quotes_mapping_fname}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_former</th>\n",
       "      <th>test_quote</th>\n",
       "      <th>test_latter</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>archived video of that message here: biblical ...</td>\n",
       "      <td>a law unto themselves.</td>\n",
       "      <td>which shew the work of the law written in the...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in the memories of his people, in the rest of ...</td>\n",
       "      <td>blessed are the meek: for they shall inherit t...</td>\n",
       "      <td>\"blessed are the merciful! for they shall obt...</td>\n",
       "      <td>867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the human species depends on maximizing the po...</td>\n",
       "      <td>injustice anywhere is a threat to justice ever...</td>\n",
       "      <td>but we do no even have to be certain of this....</td>\n",
       "      <td>2784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>said that he was 'carnal, sold under sin' . no...</td>\n",
       "      <td>for the good that i would i do not; but the ev...</td>\n",
       "      <td>this is my experience. i do many things i oug...</td>\n",
       "      <td>1545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a female student run up to a man wearing a sik...</td>\n",
       "      <td>all we are saying is give peace a chance.</td>\n",
       "      <td>over the exhortations of the speaker, the vid...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>examine the language of the church charters, r...</td>\n",
       "      <td>no religious test shall ever be required as a ...</td>\n",
       "      <td>under the united states.\" the provision is bi...</td>\n",
       "      <td>3538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>soon enough that we were not catholics, and wh...</td>\n",
       "      <td>out of the frying pan into the fire.</td>\n",
       "      <td>with vengeance.\" \"i think we might pass as sp...</td>\n",
       "      <td>3802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>her debut.\" martin talbot, of the official cha...</td>\n",
       "      <td>whatever people say i am, that's what i'm not.</td>\n",
       "      <td>source: official uk charts company sales tren...</td>\n",
       "      <td>5674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>her over, 'till she's as desp'rate to recover ...</td>\n",
       "      <td>he that complies against his will is of his ow...</td>\n",
       "      <td>which he may adhere to, yet disown, for reaso...</td>\n",
       "      <td>1872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>was the opportunity. he said unto him, 'which?...</td>\n",
       "      <td>love thy neighbor as thyself.</td>\n",
       "      <td>he did not say to him: \"you must believe in m...</td>\n",
       "      <td>3254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12771 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          test_former  \\\n",
       "0   archived video of that message here: biblical ...   \n",
       "1   in the memories of his people, in the rest of ...   \n",
       "2   the human species depends on maximizing the po...   \n",
       "3   said that he was 'carnal, sold under sin' . no...   \n",
       "4   a female student run up to a man wearing a sik...   \n",
       "..                                                ...   \n",
       "30  examine the language of the church charters, r...   \n",
       "31  soon enough that we were not catholics, and wh...   \n",
       "32  her debut.\" martin talbot, of the official cha...   \n",
       "33  her over, 'till she's as desp'rate to recover ...   \n",
       "34  was the opportunity. he said unto him, 'which?...   \n",
       "\n",
       "                                           test_quote  \\\n",
       "0                              a law unto themselves.   \n",
       "1   blessed are the meek: for they shall inherit t...   \n",
       "2   injustice anywhere is a threat to justice ever...   \n",
       "3   for the good that i would i do not; but the ev...   \n",
       "4           all we are saying is give peace a chance.   \n",
       "..                                                ...   \n",
       "30  no religious test shall ever be required as a ...   \n",
       "31               out of the frying pan into the fire.   \n",
       "32     whatever people say i am, that's what i'm not.   \n",
       "33  he that complies against his will is of his ow...   \n",
       "34                      love thy neighbor as thyself.   \n",
       "\n",
       "                                          test_latter    gs  \n",
       "0    which shew the work of the law written in the...    76  \n",
       "1    \"blessed are the merciful! for they shall obt...   867  \n",
       "2    but we do no even have to be certain of this....  2784  \n",
       "3    this is my experience. i do many things i oug...  1545  \n",
       "4    over the exhortations of the speaker, the vid...   320  \n",
       "..                                                ...   ...  \n",
       "30   under the united states.\" the provision is bi...  3538  \n",
       "31   with vengeance.\" \"i think we might pass as sp...  3802  \n",
       "32   source: official uk charts company sales tren...  5674  \n",
       "33   which he may adhere to, yet disown, for reaso...  1872  \n",
       "34   he did not say to him: \"you must believe in m...  3254  \n",
       "\n",
       "[12771 rows x 4 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.DataFrame({\"test_former\": test_former, \"test_quote\":test_quote, \"test_latter\": test_latter, \"gs\": final_df[\"gs\"]})\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate the files for sharing\n",
    "final_df_trunc = final_df.head(500)\n",
    "test_data_trunc = test_data.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharing_dirname = \"share\"\n",
    "\n",
    "# quotes for mapping\n",
    "final_df_trunc.to_parquet(f\"{sharing_dirname}/{quote_retrievals_fname}.parquet\", index=False)\n",
    "quotes_mapping.to_parquet(f\"{sharing_dirname}/{quotes_mapping_fname}.parquet\", index=False)\n",
    "# test data\n",
    "test_data_fname = \"test_data\"\n",
    "test_data_trunc.to_parquet(f\"{sharing_dirname}/{test_data_fname}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
